% Encoding: UTF-8

@article{Rudin2019LessonFromAICompetition,
  journal   = {Harvard Data Science Review},
  doi       = {10.1162/99608f92.5a8a3a3d},
  number    = {2},
  note      = {https://hdsr.mitpress.mit.edu/pub/f9kuryi8},
  publisher = {},
  title     = {Why Are We Using Black Box Models in AI When We Don’t Need To? A Lesson From An Explainable AI Competition},
  url       = {https://hdsr.mitpress.mit.edu/pub/f9kuryi8},
  volume    = {1},
  author    = {Rudin, Cynthia and Radin, Joanna},
  date      = {2019-11-22},
  year      = {2019},
  month     = {11},
  day       = {22}
}


@misc{EU2018GDPR,
  title  = {2018 reform of EU data protection rules},
  url    = {https://eur-lex.europa.eu/eli/reg/2016/679/2016-05-04},
  author = {{European Commission}},
  date   = {2016-04-27},
  year   = {2016},
  month  = {04}
}

@article{Rajpurkar2017CheXNet,
  author     = {Pranav Rajpurkar and
                Jeremy Irvin and
                Kaylie Zhu and
                Brandon Yang and
                Hershel Mehta and
                Tony Duan and
                Daisy Yi Ding and
                Aarti Bagul and
                Curtis P. Langlotz and
                Katie S. Shpanskaya and
                Matthew P. Lungren and
                Andrew Y. Ng},
  title      = {CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with
                Deep Learning},
  journal    = {CoRR},
  volume     = {abs/1711.05225},
  year       = {2017},
  url        = {http://arxiv.org/abs/1711.05225},
  eprinttype = {arXiv},
  eprint     = {1711.05225},
  timestamp  = {Fri, 26 Nov 2021 17:17:06 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1711-05225.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{Rajpurkar2018CheXNeXt,
  doi       = {10.1371/journal.pmed.1002686},
  author    = {Rajpurkar, Pranav AND Irvin, Jeremy AND Ball, Robyn L. AND Zhu, Kaylie AND Yang, Brandon AND Mehta, Hershel AND Duan, Tony AND Ding, Daisy AND Bagul, Aarti AND Langlotz, Curtis P. AND Patel, Bhavik N. AND Yeom, Kristen W. AND Shpanskaya, Katie AND Blankenberg, Francis G. AND Seekins, Jayne AND Amrhein, Timothy J. AND Mong, David A. AND Halabi, Safwan S. AND Zucker, Evan J. AND Ng, Andrew Y. AND Lungren, Matthew P.},
  journal   = {PLOS Medicine},
  publisher = {Public Library of Science},
  title     = {Deep learning for chest radiograph diagnosis: A retrospective comparison of the CheXNeXt algorithm to practicing radiologists},
  year      = {2018},
  month     = {11},
  volume    = {15},
  url       = {https://doi.org/10.1371/journal.pmed.1002686},
  pages     = {1-17},
  number    = {11}
}


@article{Waite2017RadiologistError,
  doi       = {10.2214/ajr.16.16963},
  url       = {https://doi.org/10.2214/ajr.16.16963},
  year      = {2017},
  month     = apr,
  publisher = {American Roentgen Ray Society},
  volume    = {208},
  number    = {4},
  pages     = {739--749},
  author    = {Stephen Waite and Jinel Scott and Brian Gale and Travis Fuchs and Srinivas Kolla and Deborah Reede},
  title     = {Interpretive Error in Radiology},
  journal   = {American Journal of Roentgenology}
}

@article{Sunshine2004RadiologyShortageUS,
  doi       = {10.2214/ajr.182.2.1820301},
  url       = {https://doi.org/10.2214/ajr.182.2.1820301},
  year      = {2004},
  month     = feb,
  publisher = {American Roentgen Ray Society},
  volume    = {182},
  number    = {2},
  pages     = {301--305},
  author    = {Jonathan H. Sunshine and C. Douglas Maynard and Joan Paros and Howard P. Forman},
  title     = {Update on the Diagnostic Radiologist Shortage},
  journal   = {American Journal of Roentgenology}
}


@article{Rimmer2017RadiologistShortageUk,
  doi       = {10.1136/bmj.j4683},
  url       = {https://doi.org/10.1136/bmj.j4683},
  year      = {2017},
  month     = oct,
  publisher = {{BMJ}},
  pages     = {j4683},
  author    = {Abi Rimmer},
  title     = {Radiologist shortage leaves patient care at risk,  warns royal college},
  journal   = {{BMJ}}
}


@inproceedings{Liu2019ReportGeneration,
  title     = {Clinically Accurate Chest X-Ray Report Generation},
  author    = {Liu, Guanxiong and Hsu, Tzu-Ming Harry and McDermott, Matthew and Boag, Willie and Weng, Wei-Hung and Szolovits, Peter and Ghassemi, Marzyeh},
  booktitle = {Proceedings of the 4th Machine Learning for Healthcare Conference},
  pages     = {249--269},
  year      = {2019},
  editor    = {Doshi-Velez, Finale and Fackler, Jim and Jung, Ken and Kale, David and Ranganath, Rajesh and Wallace, Byron and Wiens, Jenna},
  volume    = {106},
  series    = {Proceedings of Machine Learning Research},
  month     = {09--10 Aug},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v106/liu19a/liu19a.pdf},
  url       = {https://proceedings.mlr.press/v106/liu19a.html}
}

@inproceedings{SeyyedKalantari2020CheXclusion,
  doi       = {10.1142/9789811232701_0022},
  url       = {https://doi.org/10.1142/9789811232701_0022},
  year      = {2020},
  month     = nov,
  publisher = {{WORLD} {SCIENTIFIC}},
  author    = {Laleh Seyyed-Kalantari and Guanxiong Liu and Matthew McDermott and Irene Y. Chen and Marzyeh Ghassemi},
  title     = {{CheXclusion}: Fairness gaps in deep chest X-ray classifiers},
  booktitle = {Biocomputing 2021}
}


@article{Ribeiro2016LIME,
  author     = {Marco T{\'{u}}lio Ribeiro and
                Sameer Singh and
                Carlos Guestrin},
  title      = {"Why Should {I} Trust You?": Explaining the Predictions of Any Classifier},
  journal    = {CoRR},
  volume     = {abs/1602.04938},
  year       = {2016},
  url        = {http://arxiv.org/abs/1602.04938},
  eprinttype = {arXiv},
  eprint     = {1602.04938},
  timestamp  = {Mon, 13 Aug 2018 16:49:09 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/RibeiroSG16.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}


@article{Lundberg2017SHAP,
  author     = {Scott M. Lundberg and
                Su{-}In Lee},
  title      = {A unified approach to interpreting model predictions},
  journal    = {CoRR},
  volume     = {abs/1705.07874},
  year       = {2017},
  url        = {http://arxiv.org/abs/1705.07874},
  eprinttype = {arXiv},
  eprint     = {1705.07874},
  timestamp  = {Fri, 26 Nov 2021 16:33:36 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/LundbergL17.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{Moreira2021LINDA,
  doi       = {10.1016/j.dss.2021.113561},
  url       = {https://doi.org/10.1016/j.dss.2021.113561},
  year      = {2021},
  month     = nov,
  publisher = {Elsevier {BV}},
  volume    = {150},
  pages     = {113561},
  author    = {Catarina Moreira and Yu-Liang Chou and Mythreyi Velmurugan and Chun Ouyang and Renuka Sindhgatta and Peter Bruza},
  title     = {{LINDA}-{BN}: An interpretable probabilistic approach for demystifying black-box predictive models},
  journal   = {Decision Support Systems}
}

@article{Ahsan2020LIMEOnCXR,
  doi       = {10.3390/make2040027},
  url       = {https://doi.org/10.3390/make2040027},
  year      = {2020},
  month     = oct,
  publisher = {{MDPI} {AG}},
  volume    = {2},
  number    = {4},
  pages     = {490--504},
  author    = {Md Manjurul Ahsan and Kishor Datta Gupta and Mohammad Maminur Islam and Sajib Sen and Md. Lutfar Rahman and Mohammad Shakhawat Hossain},
  title     = {{COVID}-19 Symptoms Detection Based on {NasNetMobile} with Explainable {AI} Using Various Imaging Modalities},
  journal   = {Machine Learning and Knowledge Extraction}
}

@article{Teixeira2021LIMEAndGradCAMOnCXR,
  doi       = {10.3390/s21217116},
  url       = {https://doi.org/10.3390/s21217116},
  year      = {2021},
  month     = oct,
  publisher = {{MDPI} {AG}},
  volume    = {21},
  number    = {21},
  pages     = {7116},
  author    = {Lucas O. Teixeira and Rodolfo M. Pereira and Diego Bertolini and Luiz S. Oliveira and Loris Nanni and George D. C. Cavalcanti and Yandre M. G. Costa},
  title     = {Impact of Lung Segmentation on the Diagnosis and Explanation of {COVID}-19 in Chest X-ray Images},
  journal   = {Sensors}
}

@incollection{Zeiler2014UnderstandCNN,
  doi       = {10.1007/978-3-319-10590-1_53},
  url       = {https://doi.org/10.1007/978-3-319-10590-1_53},
  year      = {2014},
  publisher = {Springer International Publishing},
  pages     = {818--833},
  author    = {Matthew D. Zeiler and Rob Fergus},
  title     = {Visualizing and Understanding Convolutional Networks},
  booktitle = {Computer Vision {\textendash} {ECCV} 2014}
}

@inproceedings{Simonyan14DeepInside,
  author    = {Karen Simonyan and Andrea Vedaldi and Andrew Zisserman},
  title     = {Deep inside convolutional networks: Visualising image classification models and saliency maps},
  booktitle = {In Workshop at International Conference on Learning Representations},
  year      = {2014}
}

@article{Zhou2015CAM,
  author     = {Bolei Zhou and
                Aditya Khosla and
                {\`{A}}gata Lapedriza and
                Aude Oliva and
                Antonio Torralba},
  title      = {Learning Deep Features for Discriminative Localization},
  journal    = {CoRR},
  volume     = {abs/1512.04150},
  year       = {2015},
  url        = {http://arxiv.org/abs/1512.04150},
  eprinttype = {arXiv},
  eprint     = {1512.04150},
  timestamp  = {Mon, 13 Aug 2018 16:47:46 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/ZhouKLOT15.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Selvaraju2017GradCAM,
  author    = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  title     = {Grad-CAM: Visual Explanations From Deep Networks via Gradient-Based Localization},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  month     = {Oct},
  year      = {2017}
}

@inproceedings{Chattopadhay2018GradCAM++,
  doi       = {10.1109/wacv.2018.00097},
  url       = {https://doi.org/10.1109/wacv.2018.00097},
  year      = {2018},
  month     = mar,
  publisher = {{IEEE}},
  author    = {Aditya Chattopadhay and Anirban Sarkar and Prantik Howlader and Vineeth N Balasubramanian},
  title     = {Grad-{CAM}$\mathplus$$\mathplus$: Generalized Gradient-Based Visual Explanations for Deep Convolutional Networks},
  booktitle = {2018 {IEEE} Winter Conference on Applications of Computer Vision ({WACV})}
}

@article{Saporta2021BechmarkingSaliencyMethods,
  doi       = {10.1101/2021.02.28.21252634},
  url       = {https://doi.org/10.1101/2021.02.28.21252634},
  year      = {2021},
  month     = mar,
  publisher = {Cold Spring Harbor Laboratory},
  author    = {Adriel Saporta and Xiaotong Gui and Ashwin Agrawal and Anuj Pareek and Steven QH Truong and Chanh DT Nguyen and Van-Doan Ngo and Jayne Seekins and Francis G. Blankenberg and Andrew Y. Ng and Matthew P. Lungren and Pranav Rajpurkar},
  title     = {Benchmarking saliency methods for chest X-ray interpretation}
}

@article{Sundararajan2017IntegratedGradient,
  author     = {Mukund Sundararajan and
                Ankur Taly and
                Qiqi Yan},
  title      = {Axiomatic Attribution for Deep Networks},
  journal    = {CoRR},
  volume     = {abs/1703.01365},
  year       = {2017},
  url        = {http://arxiv.org/abs/1703.01365},
  eprinttype = {arXiv},
  eprint     = {1703.01365},
  timestamp  = {Mon, 13 Aug 2018 16:48:32 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/SundararajanTY17.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Ngiam2011MultiModalLearning,
  author    = {Jiquan Ngiam and Aditya Khosla and Mingyu Kim and Juhan Nam and Honglak Lee and Andrew Y. Ng},
  title     = {Multimodal Deep Learning},
  year      = {2011},
  cdate     = {1293840000000},
  pages     = {689-696},
  url       = {https://icml.cc/2011/papers/399_icmlpaper.pdf},
  booktitle = {ICML},
  crossref  = {conf/icml/2011}
}

@article{Karargyris2021EyeGazePaper,
  doi       = {10.1038/s41597-021-00863-5},
  url       = {https://doi.org/10.1038/s41597-021-00863-5},
  year      = {2021},
  month     = mar,
  publisher = {Springer Science and Business Media {LLC}},
  volume    = {8},
  number    = {1},
  author    = {Alexandros Karargyris and Satyananda Kashyap and Ismini Lourentzou and Joy T. Wu and Arjun Sharma and Matthew Tong and Shafiq Abedin and David Beymer and Vandana Mukherjee and Elizabeth A. Krupinski and Mehdi Moradi},
  title     = {Creation and validation of a chest X-ray dataset with eye-tracking and report dictation for {AI} development},
  journal   = {Scientific Data}
}


@misc{Johnson2021MIMIC_IV,
  doi       = {10.13026/S6N6-XD98},
  url       = {https://physionet.org/content/mimiciv/1.0/},
  author    = {Johnson,  Alistair and Bulgarelli,  Lucas and Pollard,  Tom and Horng,  Steven and Celi,  Leo Anthony and Mark,  Roger},
  title     = {MIMIC-IV},
  publisher = {PhysioNet},
  year      = {2021}
}

@misc{Johnson2021MIMIC_IV_ED,
  doi       = {10.13026/77Z6-9W59},
  url       = {https://physionet.org/content/mimic-iv-ed/1.0/},
  author    = {Johnson,  Alistair and Bulgarelli,  Lucas and Pollard,  Tom and Celi,  Leo Anthony and Mark,  Roger and Horng,  Steven},
  title     = {MIMIC-IV-ED},
  publisher = {PhysioNet},
  year      = {2021}
}

@misc{Johnson2019MIMIC_CXR,
  doi       = {10.13026/C2JT1Q},
  url       = {https://physionet.org/content/mimic-cxr/},
  author    = {Johnson,  Alistair E. W. and Pollard,  Tom and Mark,  Roger and Berkowitz,  Seth and Horng,  Steven},
  title     = {The MIMIC-CXR Database},
  publisher = {physionet.org},
  year      = {2019}
}

@article{DJohnson2019MIMIC_CXR_JPG,
  author     = {Alistair E. W. Johnson and
                Tom J. Pollard and
                Seth J. Berkowitz and
                Nathaniel R. Greenbaum and
                Matthew P. Lungren and
                Chih{-}ying Deng and
                Roger G. Mark and
                Steven Horng},
  title      = {{MIMIC-CXR-JPG:} {A} large publicly available database of labeled chest
                radiographs},
  journal    = {CoRR},
  volume     = {abs/1901.07042},
  year       = {2019},
  url        = {http://arxiv.org/abs/1901.07042},
  eprinttype = {arXiv},
  eprint     = {1901.07042},
  timestamp  = {Thu, 14 Oct 2021 09:14:23 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1901-07042.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}


@misc{Karargyris2020EyeGazeDataset,
  author    = {Karargyris, Alexandros and Kashyap, Satyananda and Lourentzou, Ismini and Wu, Joy and Tong, Matthew and Sharma, Arjun and Abedin, Shafiq and Beymer, David and Mukherjee, Vandana and Krupinski, Elizabeth and Moradi, Mehdi},
  booktitle = {Physionet},
  doi       = {https://doi.org/10.13026/qfdz-zr67},
  title     = {{Eye Gaze Data for Chest X-rays (version 1.0.0)}},
  url       = {https://physionet.org/content/egd-cxr/1.0.0/},
  year      = {2020}
}

@misc{Lanfredi2021REFLACX,
  doi       = {10.13026/E0DJ-8498},
  url       = {https://physionet.org/content/reflacx-xray-localization/1.0.0/},
  author    = {Bigolin Lanfredi,  Ricardo and Zhang,  Mingyuan and Auffermann,  William and Chan,  Jessica and Duong,  Phuong-Anh and Srikumar,  Vivek and Drew,  Trafton and Schroeder,  Joyce and Tasdizen,  Tolga},
  title     = {REFLACX: Reports and eye-tracking data for localization of abnormalities in chest x-rays},
  publisher = {PhysioNet},
  year      = {2021}
}

@article{LoyolaGonzalez2019BlackBoxVsWhiteBox,
  doi       = {10.1109/access.2019.2949286},
  url       = {https://doi.org/10.1109/access.2019.2949286},
  year      = {2019},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume    = {7},
  pages     = {154096--154113},
  author    = {Octavio Loyola-Gonzalez},
  title     = {Black-Box vs. White-Box: Understanding Their Advantages and Weaknesses From a Practical Point of View},
  journal   = {{IEEE} Access}
}

@misc{doshivelez2017RigorousInterpretable,
  title         = {Towards A Rigorous Science of Interpretable Machine Learning},
  author        = {Finale Doshi-Velez and Been Kim},
  year          = {2017},
  eprint        = {1702.08608},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML}
}

@article{Baltrusaitis2019MutimodalSurvey,
  author  = {Baltrušaitis, Tadas and Ahuja, Chaitanya and Morency, Louis-Philippe},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title   = {Multimodal Machine Learning: A Survey and Taxonomy},
  year    = {2019},
  volume  = {41},
  number  = {2},
  pages   = {423-443},
  doi     = {10.1109/TPAMI.2018.2798607}
}

@article{Ruder2017MultitaskLearning,
  author     = {Sebastian Ruder},
  title      = {An Overview of Multi-Task Learning in Deep Neural Networks},
  journal    = {CoRR},
  volume     = {abs/1706.05098},
  year       = {2017},
  url        = {http://arxiv.org/abs/1706.05098},
  eprinttype = {arXiv},
  eprint     = {1706.05098},
  timestamp  = {Mon, 13 Aug 2018 16:48:50 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/Ruder17a.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}


@article{Belle2020XAIPriciples,
  author     = {Vaishak Belle and
                Ioannis Papantonis},
  title      = {Principles and Practice of Explainable Machine Learning},
  journal    = {CoRR},
  volume     = {abs/2009.11698},
  year       = {2020},
  url        = {https://arxiv.org/abs/2009.11698},
  eprinttype = {arXiv},
  eprint     = {2009.11698},
  timestamp  = {Wed, 30 Sep 2020 16:16:22 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2009-11698.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Karlo2018XAISurvey,
  author    = {Došilović, Filip Karlo and Brčić, Mario and Hlupić, Nikica},
  booktitle = {2018 41st International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)},
  title     = {Explainable artificial intelligence: A survey},
  year      = {2018},
  volume    = {},
  number    = {},
  pages     = {0210-0215},
  doi       = {10.23919/MIPRO.2018.8400040}
}

@misc{Ribeiro2016Modelagnostic,
  title         = {Model-Agnostic Interpretability of Machine Learning},
  author        = {Marco Tulio Ribeiro and Sameer Singh and Carlos Guestrin},
  year          = {2016},
  eprint        = {1606.05386},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML}
}

@article{Ribeiro2018Anchors,
  title   = {Anchors: High-Precision Model-Agnostic Explanations},
  volume  = {32},
  url     = {https://ojs.aaai.org/index.php/AAAI/article/view/11491},
  number  = {1},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  author  = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year    = {2018},
  month   = {Apr.}
}

@inbook{Charnes1988ShapeleyValueInEconomic,
  author    = {Charnes, A.
               and Golany, B.
               and Keane, M.
               and Rousseau, J.},
  editor    = {Sengupta, Jati K.
               and Kadekodi, Gopal K.},
  title     = {Extremal Principle Solutions of Games in Characteristic Function Form: Core, Chebychev and Shapley Value Generalizations},
  booktitle = {Econometrics of Planning and Efficiency},
  year      = {1988},
  publisher = {Springer Netherlands},
  address   = {Dordrecht},
  pages     = {123--133},
  isbn      = {978-94-009-3677-5},
  doi       = {10.1007/978-94-009-3677-5_7},
  url       = {https://doi.org/10.1007/978-94-009-3677-5_7}
}



@inproceedings{Shrikumar2017DeepLIFT,
  title     = {Learning Important Features Through Propagating Activation Differences},
  author    = {Avanti Shrikumar and Peyton Greenside and Anshul Kundaje},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  pages     = {3145--3153},
  year      = {2017},
  editor    = {Precup, Doina and Teh, Yee Whye},
  volume    = {70},
  series    = {Proceedings of Machine Learning Research},
  month     = {06--11 Aug},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v70/shrikumar17a/shrikumar17a.pdf},
  url       = {https://proceedings.mlr.press/v70/shrikumar17a.html},
  abstract  = {The purported “black box” nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its `reference activation’ and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. Video tutorial: http://goo.gl/qKb7pL code: http://goo.gl/RM8jvH}
}


@article{Spinner2020explAIner,
  author  = {Spinner, Thilo and Schlegel, Udo and Schäfer, Hanna and El-Assady, Mennatallah},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  title   = {<bold>explAIner</bold>: A Visual Analytics Framework for Interactive and Explainable Machine Learning},
  year    = {2020},
  volume  = {26},
  number  = {1},
  pages   = {1064-1074},
  doi     = {10.1109/TVCG.2019.2934629}
}



@article{Singh2020ExplainableMedicalImage,
  author         = {Singh, Amitojdeep and Sengupta, Sourya and Lakshminarayanan, Vasudevan},
  title          = {Explainable Deep Learning Models in Medical Image Analysis},
  journal        = {Journal of Imaging},
  volume         = {6},
  year           = {2020},
  number         = {6},
  article-number = {52},
  url            = {https://www.mdpi.com/2313-433X/6/6/52},
  issn           = {2313-433X},
  abstract       = {Deep learning methods have been very effective for a variety of medical diagnostic tasks and have even outperformed human experts on some of those. However, the black-box nature of the algorithms has restricted their clinical use. Recent explainability studies aim to show the features that influence the decision of a model the most. The majority of literature reviews of this area have focused on taxonomy, ethics, and the need for explanations. A review of the current applications of explainable deep learning for different medical imaging tasks is presented here. The various approaches, challenges for clinical deployment, and the areas requiring further research are discussed here from a practical standpoint of a deep learning researcher designing a system for the clinical end-users.},
  doi            = {10.3390/jimaging6060052}
}


@article{Yuan2020DAM,
  author     = {Zhuoning Yuan and
                Yan Yan and
                Milan Sonka and
                Tianbao Yang},
  title      = {Robust Deep {AUC} Maximization: {A} New Surrogate Loss and Empirical
                Studies on Medical Image Classification},
  journal    = {CoRR},
  volume     = {abs/2012.03173},
  year       = {2020},
  url        = {https://arxiv.org/abs/2012.03173},
  eprinttype = {arXiv},
  eprint     = {2012.03173},
  timestamp  = {Wed, 09 Dec 2020 15:29:05 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2012-03173.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{Huang2005AUC,
  author  = {Jin Huang and Ling, C.X.},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  title   = {Using AUC and accuracy in evaluating learning algorithms},
  year    = {2005},
  volume  = {17},
  number  = {3},
  pages   = {299-310},
  doi     = {10.1109/TKDE.2005.50}
}

@article{Lobo2008AUDIsWrong,
  doi       = {10.1111/j.1466-8238.2007.00358.x},
  url       = {https://doi.org/10.1111/j.1466-8238.2007.00358.x},
  year      = {2008},
  month     = mar,
  publisher = {Wiley},
  volume    = {17},
  number    = {2},
  pages     = {145--151},
  author    = {Jorge M. Lobo and Alberto Jim{\'{e}}nez-Valverde and Raimundo Real},
  title     = {{AUC}: a misleading measure of the performance of predictive distribution models},
  journal   = {Global Ecology and Biogeography}
}

@article{HajianTilaki2013AUDOnMedical,
  title    = {Receiver Operating Characteristic ({ROC}) Curve Analysis for
              Medical Diagnostic Test Evaluation},
  author   = {Hajian-Tilaki, Karimollah},
  abstract = {This review provides the basic principle and rational for ROC
              analysis of rating and continuous diagnostic test results versus
              a gold standard. Derived indexes of accuracy, in particular area
              under the curve (AUC) has a meaningful interpretation for disease
              classification from healthy subjects. The methods of estimate of
              AUC and its testing in single diagnostic test and also
              comparative studies, the advantage of ROC curve to determine the
              optimal cut off values and the issues of bias and confounding
              have been discussed.},
  journal  = {Caspian J Intern Med},
  volume   = 4,
  number   = 2,
  pages    = {627--635},
  year     = 2013,
  keywords = {Area under the curve (AUC); Bias; Nonparametric; Parametric; ROC
              curve; Sensitivity; Specificity},
  language = {en}
}


%%% Need to include 

@article{Linardatos2021ExplainaleAIReview,
  author         = {Linardatos, Pantelis and Papastefanopoulos, Vasilis and Kotsiantis, Sotiris},
  title          = {Explainable AI: A Review of Machine Learning Interpretability Methods},
  journal        = {Entropy},
  volume         = {23},
  year           = {2021},
  number         = {1},
  article-number = {18},
  url            = {https://www.mdpi.com/1099-4300/23/1/18},
  pubmedid       = {33375658},
  issn           = {1099-4300},
  doi            = {10.3390/e23010018}
}

@article{Gao2012SSurrogatelossAUC,
  author     = {Wei Gao and
                Zhi{-}Hua Zhou},
  title      = {On the consistency of {AUC} Optimization},
  journal    = {CoRR},
  volume     = {abs/1208.0645},
  year       = {2012},
  url        = {http://arxiv.org/abs/1208.0645},
  eprinttype = {arXiv},
  eprint     = {1208.0645},
  timestamp  = {Mon, 13 Aug 2018 16:48:08 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1208-0645.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{Gao2013AUCSquareLoss,
  title     = {One-Pass AUC Optimization},
  author    = {Gao, Wei and Jin, Rong and Zhu, Shenghuo and Zhou, Zhi-Hua},
  booktitle = {Proceedings of the 30th International Conference on Machine Learning},
  pages     = {906--914},
  year      = {2013},
  editor    = {Dasgupta, Sanjoy and McAllester, David},
  volume    = {28},
  number    = {3},
  series    = {Proceedings of Machine Learning Research},
  address   = {Atlanta, Georgia, USA},
  month     = {17--19 Jun},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v28/gao13.pdf},
  url       = {https://proceedings.mlr.press/v28/gao13.html},
  abstract  = {AUC is an important performance measure and many algorithms have been devoted to AUC optimization, mostly by minimizing a surrogate convex loss on a training data set. In this work, we focus on one-pass AUC optimization that requires only going through the training data once without storing the entire training dataset, where conventional online learning algorithms cannot be applied directly because AUC is measured by a sum of losses defined over pairs of instances from different classes. We develop a regression-based algorithm which only needs to maintain the first and second order statistics of training data in memory, resulting a storage requirement independent from the size of training data. To efficiently handle high dimensional data, we develop a randomized algorithm that approximates the covariance matrices by low rank matrices. We verify, both theoretically and empirically, the effectiveness of the proposed algorithm.}
}

@inproceedings{Sulam2017MaximizingAUD,
  title     = {Maximizing AUC with Deep Learning for Classification of Imbalanced Mammogram Datasets.},
  author    = {Sulam, Jeremias and Ben-Ari, Rami and Kisilev, Pavel},
  booktitle = {VCBM},
  pages     = {131--135},
  year      = {2017}
}

@misc{Irvin2019Chexpert,
  title         = {CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison},
  author        = {Jeremy Irvin and Pranav Rajpurkar and Michael Ko and Yifan Yu and Silviana Ciurea-Ilcus and Chris Chute and Henrik Marklund and Behzad Haghgoo and Robyn Ball and Katie Shpanskaya and Jayne Seekins and David A. Mong and Safwan S. Halabi and Jesse K. Sandberg and Ricky Jones and David B. Larson and Curtis P. Langlotz and Bhavik N. Patel and Matthew P. Lungren and Andrew Y. Ng},
  year          = {2019},
  eprint        = {1901.07031},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@article{Huang2016DenseNet,
  author     = {Gao Huang and
                Zhuang Liu and
                Kilian Q. Weinberger},
  title      = {Densely Connected Convolutional Networks},
  journal    = {CoRR},
  volume     = {abs/1608.06993},
  year       = {2016},
  url        = {http://arxiv.org/abs/1608.06993},
  eprinttype = {arXiv},
  eprint     = {1608.06993},
  timestamp  = {Mon, 10 Sep 2018 15:49:32 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/HuangLW16a.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{He2015ResNet,
  author     = {Kaiming He and
                Xiangyu Zhang and
                Shaoqing Ren and
                Jian Sun},
  title      = {Deep Residual Learning for Image Recognition},
  journal    = {CoRR},
  volume     = {abs/1512.03385},
  year       = {2015},
  url        = {http://arxiv.org/abs/1512.03385},
  eprinttype = {arXiv},
  eprint     = {1512.03385},
  timestamp  = {Wed, 17 Apr 2019 17:23:45 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/HeZRS15.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}


@article{Cohen2020Covid19,
  title    = {Predicting {COVID-19} Pneumonia Severity on Chest X-ray With Deep
              Learning},
  author   = {Cohen, Joseph Paul and Dao, Lan and Roth, Karsten and Morrison,
              Paul and Bengio, Yoshua and Abbasi, Almas F and Shen, Beiyi and
              Mahsa, Hoshmand Kochi and Ghassemi, Marzyeh and Li, Haifang and
              Duong, Tim Q},
  abstract = {Introduction The need to streamline patient management for
              coronavirus disease-19 (COVID-19) has become more pressing than
              ever. Chest X-rays (CXRs) provide a non-invasive (potentially
              bedside) tool to monitor the progression of the disease. In this
              study, we present a severity score prediction model for COVID-19
              pneumonia for frontal chest X-ray images. Such a tool can gauge
              the severity of COVID-19 lung infections (and pneumonia in
              general) that can be used for escalation or de-escalation of care
              as well as monitoring treatment efficacy, especially in the ICU.
              Methods Images from a public COVID-19 database were scored
              retrospectively by three blinded experts in terms of the extent
              of lung involvement as well as the degree of opacity. A neural
              network model that was pre-trained on large (non-COVID-19) chest
              X-ray datasets is used to construct features for COVID-19 images
              which are predictive for our task. Results This study finds that
              training a regression model on a subset of the outputs from this
              pre-trained chest X-ray model predicts our geographic extent
              score (range 0-8) with 1.14 mean absolute error (MAE) and our
              lung opacity score (range 0-6) with 0.78 MAE. Conclusions These
              results indicate that our model's ability to gauge the severity
              of COVID-19 lung infections could be used for escalation or
              de-escalation of care as well as monitoring treatment efficacy,
              especially in the ICU. To enable follow up work, we make our
              code, labels, and data available online.},
  journal  = {Cureus},
  volume   = 12,
  number   = 7,
  pages    = {e9448},
  month    = jul,
  year     = 2020,
  keywords = {chest x-ray; covid-19 pneumonia; deep learning artificial
              intelligence; severity scoring},
  language = {en}
}

@inproceedings{Deng2009ImageNet,
  author    = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
  booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
  title     = {ImageNet: A large-scale hierarchical image database},
  year      = {2009},
  volume    = {},
  number    = {},
  pages     = {248-255},
  doi       = {10.1109/CVPR.2009.5206848}
}

@article{Dunnmon2019CNNOnCXRAssessment,
  doi       = {10.1148/radiol.2018181422},
  url       = {https://doi.org/10.1148/radiol.2018181422},
  year      = {2019},
  month     = feb,
  publisher = {Radiological Society of North America ({RSNA})},
  volume    = {290},
  number    = {2},
  pages     = {537--544},
  author    = {Jared A. Dunnmon and Darvin Yi and Curtis P. Langlotz and Christopher R{\'{e}} and Daniel L. Rubin and Matthew P. Lungren},
  title     = {Assessment of Convolutional Neural Networks for Automated Classification of Chest Radiographs},
  journal   = {Radiology}
}

@inproceedings{Alex2012AlexNet,
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {ImageNet Classification with Deep Convolutional Neural Networks},
  url       = {https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
  volume    = {25},
  year      = {2012}
}

@article{Zhuang2019TransferLearningSurvey,
  author     = {Fuzhen Zhuang and
                Zhiyuan Qi and
                Keyu Duan and
                Dongbo Xi and
                Yongchun Zhu and
                Hengshu Zhu and
                Hui Xiong and
                Qing He},
  title      = {A Comprehensive Survey on Transfer Learning},
  journal    = {CoRR},
  volume     = {abs/1911.02685},
  year       = {2019},
  url        = {http://arxiv.org/abs/1911.02685},
  eprinttype = {arXiv},
  eprint     = {1911.02685},
  timestamp  = {Sat, 29 Aug 2020 18:19:14 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1911-02685.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{Lin2014MSCOCO,
  author     = {Tsung{-}Yi Lin and
                Michael Maire and
                Serge J. Belongie and
                Lubomir D. Bourdev and
                Ross B. Girshick and
                James Hays and
                Pietro Perona and
                Deva Ramanan and
                Piotr Doll{\'{a}}r and
                C. Lawrence Zitnick},
  title      = {Microsoft {COCO:} Common Objects in Context},
  journal    = {CoRR},
  volume     = {abs/1405.0312},
  year       = {2014},
  url        = {http://arxiv.org/abs/1405.0312},
  eprinttype = {arXiv},
  eprint     = {1405.0312},
  timestamp  = {Mon, 13 Aug 2018 16:48:13 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/LinMBHPRDZ14.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{Kuznetsova2020OpenImages,
  author  = {Alina Kuznetsova and Hassan Rom and Neil Alldrin and Jasper Uijlings and Ivan Krasin and Jordi Pont-Tuset and Shahab Kamali and Stefan Popov and Matteo Malloci and Alexander Kolesnikov and Tom Duerig and Vittorio Ferrari},
  title   = {The Open Images Dataset V4: Unified image classification, object detection, and visual relationship detection at scale},
  year    = {2020},
  journal = {IJCV}
}


@article{Brunese2020ExplainableCovid19,
  title    = {Explainable Deep Learning for Pulmonary Disease and Coronavirus COVID-19 Detection from X-rays},
  journal  = {Computer Methods and Programs in Biomedicine},
  volume   = {196},
  pages    = {105608},
  year     = {2020},
  issn     = {0169-2607},
  doi      = {https://doi.org/10.1016/j.cmpb.2020.105608},
  url      = {https://www.sciencedirect.com/science/article/pii/S0169260720314413},
  author   = {Luca Brunese and Francesco Mercaldo and Alfonso Reginelli and Antonella Santone},
  keywords = {Coronavirus, COVID-19, Chest, Deep learning, Transfer learning, Artificial intelligence},
  abstract = {Background and Objective: Coronavirus disease (COVID-19) is an infectious disease caused by a new virus never identified before in humans. This virus causes respiratory disease (for instance, flu) with symptoms such as cough, fever and, in severe cases, pneumonia. The test to detect the presence of this virus in humans is performed on sputum or blood samples and the outcome is generally available within a few hours or, at most, days. Analysing biomedical imaging the patient shows signs of pneumonia. In this paper, with the aim of providing a fully automatic and faster diagnosis, we propose the adoption of deep learning for COVID-19 detection from X-rays. Method: In particular, we propose an approach composed by three phases: the first one to detect if in a chest X-ray there is the presence of a pneumonia. The second one to discern between COVID-19 and pneumonia. The last step is aimed to localise the areas in the X-ray symptomatic of the COVID-19 presence. Results and Conclusion: Experimental analysis on 6,523 chest X-rays belonging to different institutions demonstrated the effectiveness of the proposed approach, with an average time for COVID-19 detection of approximately 2.5 seconds and an average accuracy equal to 0.97.}
}

@article{Hou2021ExplainableCovid19,
  doi       = {10.1038/s41598-021-95680-6},
  url       = {https://doi.org/10.1038/s41598-021-95680-6},
  year      = {2021},
  month     = aug,
  publisher = {Springer Science and Business Media {LLC}},
  volume    = {11},
  number    = {1},
  author    = {Jie Hou and Terry Gao},
  title     = {Explainable {DCNN} based chest X-ray image analysis and classification for {COVID}-19 pneumonia detection},
  journal   = {Scientific Reports}
}


@article{Harezlak2018EyeTrackingInMedicine,
  title    = {Application of eye tracking in medicine: A survey, research issues and challenges},
  journal  = {Computerized Medical Imaging and Graphics},
  volume   = {65},
  pages    = {176-190},
  year     = {2018},
  note     = {Advances in Biomedical Image Processing},
  issn     = {0895-6111},
  doi      = {https://doi.org/10.1016/j.compmedimag.2017.04.006},
  url      = {https://www.sciencedirect.com/science/article/pii/S0895611117300435},
  author   = {Katarzyna Harezlak and Pawel Kasprowski},
  keywords = {Eye tracking medicine gaze},
  abstract = {The performance and quality of medical procedures and treatments are inextricably linked to technological development. The application of more advanced techniques provides the opportunity to gain wider knowledge and deeper understanding of the human body and mind functioning. The eye tracking methods used to register eye movement to find the direction and targets of a person's gaze are well in line with the nature of the topic. By providing methods for capturing and processing images of the eye it has become possible not only to reveal abnormalities in eye functioning but also to conduct cognitive studies focused on learning about peoples’ emotions and intentions. The usefulness of the application of eye tracking technology in medicine was proved in many research studies. The aim of this paper is to give an insight into those studies and the way they utilize eye imaging in medical applications. These studies were differentiated taking their purpose and experimental paradigms into account. Additionally, methods for eye movement visualization and metrics for its quantifying were presented. Apart from presenting the state of the art, the aim of the paper was also to point out possible applications of eye tracking in medicine that have not been exhaustively investigated yet, and are going to be a perspective long-term direction of research.}
}

@article{Castillo2020ClinicalInformationOnRadiology,
  doi       = {10.1002/jmrs.424},
  url       = {https://doi.org/10.1002/jmrs.424},
  year      = {2020},
  month     = sep,
  publisher = {Wiley},
  volume    = {68},
  number    = {1},
  pages     = {60--74},
  author    = {Chelsea Castillo and Tom Steffens and Lawrence Sim and Liam Caffery},
  title     = {The effect of clinical information on radiology reporting: A systematic review},
  journal   = {Journal of Medical Radiation Sciences}
}


@inproceedings{Blascheck2014VisualisingEyeTracking,
  title     = {State-of-the-Art of Visualization for Eye Tracking Data.},
  author    = {Blascheck, Tanja and Kurzhals, Kuno and Raschke, Michael and Burch, Michael and Weiskopf, Daniel and Ertl, Thomas},
  booktitle = {EuroVis (STARs)},
  year      = {2014}
}

@article{Cooke2005EyeTrackingUsability,
  author   = {Cooke,Lynne},
  year     = {2005},
  month    = {11},
  title    = {Eye Tracking: How It Works and How It Relates to Usability},
  journal  = {Technical Communication},
  volume   = {52},
  number   = {4},
  pages    = {456-463},
  note     = {Name - Association for Computing Machinery; Copyright - Copyright Society for Technical Communication Nov 2005; Document feature - ; Last updated - 2019-11-23; CODEN - TLCMBT; SubjectsTermNotLitGenreText - New York},
  abstract = {Cooke investigates how eye tracking works and how it relates to usability. Suggested areas for future eye-tracking research are also discussed.},
  keywords = {Printing; Digital electronics; Eyes & eyesight; Tracking control systems; Eye movements; New York},
  isbn     = {00493155},
  language = {English},
  url      = {https://gateway.library.qut.edu.au/login?url=https://www.proquest.com/scholarly-journals/eye-tracking-how-works-relates-usability/docview/220995671/se-2?accountid=13380}
}

@article{Fitts2005Cockpit,
  title     = {Eye movements of aircraft pilots during instrument-landing approaches},
  author    = {Fitts, Paul M and Jones, Richard E and Milton, John L},
  journal   = {Ergonomics: Psychological mechanisms and models in ergonomics},
  volume    = {3},
  pages     = {56},
  year      = {2005},
  publisher = {Taylor \& Francis}
}

@incollection{Schall2014EyeTrackingIntro,
  title     = {Introduction to Eye Tracking},
  editor    = {Jennifer {Romano Bergstrom} and Andrew Jonathan Schall},
  booktitle = {Eye Tracking in User Experience Design},
  publisher = {Morgan Kaufmann},
  address   = {Boston},
  pages     = {3-26},
  year      = {2014},
  isbn      = {978-0-12-408138-3},
  doi       = {https://doi.org/10.1016/B978-0-12-408138-3.00001-7},
  url       = {https://www.sciencedirect.com/science/article/pii/B9780124081383000017},
  author    = {Andrew Schall and Jennifer {Romano Bergstrom}},
  keywords  = {eye tracker, eye tracking, history of eye tracking, user experience, user research, UX, vision science},
  abstract  = {This chapter contains a brief history of eye tracking and how it has become a valuable methodology for user experience researchers. Readers will gain a basic understanding of how eye trackers can track the location of a user’s eye gaze and common visualizations used to analyze the eye-tracking data output.}
}

@article{Yates2018RedDotCXR,
  title    = {Machine learning “red dot”: open-source, cloud, deep convolutional neural networks in chest radiograph binary normality classification},
  journal  = {Clinical Radiology},
  volume   = {73},
  number   = {9},
  pages    = {827-831},
  year     = {2018},
  issn     = {0009-9260},
  doi      = {https://doi.org/10.1016/j.crad.2018.05.015},
  url      = {https://www.sciencedirect.com/science/article/pii/S000992601830206X},
  author   = {E.J. Yates and L.C. Yates and H. Harvey},
  abstract = {Aim
              To develop a machine learning-based model for the binary classification of chest radiography abnormalities, to serve as a retrospective tool in guiding clinician reporting prioritisation.
              Materials and methods
              The open-source machine learning library, Tensorflow, was used to retrain a final layer of the deep convolutional neural network, Inception, to perform binary normality classification on two, anonymised, public image datasets. Re-training was performed on 47,644 images using commodity hardware, with validation testing on 5,505 previously unseen radiographs. Confusion matrix analysis was performed to derive diagnostic utility metrics.
              Results
              A final model accuracy of 94.6% (95% confidence interval [CI]: 94.3–94.7%) based on an unseen testing subset (n=5,505) was obtained, yielding a sensitivity of 94.6% (95% CI: 94.4–94.7%) and a specificity of 93.4% (95% CI: 87.2–96.9%) with a positive predictive value (PPV) of 99.8% (95% CI: 99.7–99.9%) and area under the curve (AUC) of 0.98 (95% CI: 0.97–0.99).
              Conclusion
              This study demonstrates the application of a machine learning-based approach to classify chest radiographs as normal or abnormal. Its application to real-world datasets may be warranted in optimising clinician workload.}
}

@article{Szegedy2015InceptionV3,
  author     = {Christian Szegedy and
                Vincent Vanhoucke and
                Sergey Ioffe and
                Jonathon Shlens and
                Zbigniew Wojna},
  title      = {Rethinking the Inception Architecture for Computer Vision},
  journal    = {CoRR},
  volume     = {abs/1512.00567},
  year       = {2015},
  url        = {http://arxiv.org/abs/1512.00567},
  eprinttype = {arXiv},
  eprint     = {1512.00567},
  timestamp  = {Mon, 13 Aug 2018 16:49:07 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/SzegedyVISW15.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{Tang2020AbnormalityCXRCNN,
  doi       = {10.1038/s41746-020-0273-z},
  url       = {https://doi.org/10.1038/s41746-020-0273-z},
  year      = {2020},
  month     = may,
  publisher = {Springer Science and Business Media {LLC}},
  volume    = {3},
  number    = {1},
  author    = {Yu-Xing Tang and You-Bao Tang and Yifan Peng and Ke Yan and Mohammadhadi Bagheri and Bernadette A. Redd and Catherine J. Brandon and Zhiyong Lu and Mei Han and Jing Xiao and Ronald M. Summers},
  title     = {Automated abnormality classification of chest radiographs using deep convolutional neural networks},
  journal   = {npj Digital Medicine}
}

@article{Ting2017DLOnEyeDisease,
  author   = {Ting, Daniel Shu Wei and Cheung, Carol Yim-Lui and Lim, Gilbert and Tan, Gavin Siew Wei and Quang, Nguyen D. and Gan, Alfred and Hamzah, Haslina and Garcia-Franco, Renata and San Yeo, Ian Yew and Lee, Shu Yen and Wong, Edmund Yick Mun and Sabanayagam, Charumathi and Baskaran, Mani and Ibrahim, Farah and Tan, Ngiap Chuan and Finkelstein, Eric A. and Lamoureux, Ecosse L. and Wong, Ian Y. and Bressler, Neil M. and Sivaprasad, Sobha and Varma, Rohit and Jonas, Jost B. and He, Ming Guang and Cheng, Ching-Yu and Cheung, Gemmy Chui Ming and Aung, Tin and Hsu, Wynne and Lee, Mong Li and Wong, Tien Yin},
  title    = {{Development and Validation of a Deep Learning System for Diabetic Retinopathy and Related Eye Diseases Using Retinal Images From Multiethnic Populations With Diabetes}},
  journal  = {JAMA},
  volume   = {318},
  number   = {22},
  pages    = {2211-2223},
  year     = {2017},
  month    = {12},
  abstract = {{A deep learning system (DLS) is a machine learning technology with potential for screening diabetic retinopathy and related eye diseases.To evaluate the performance of a DLS in detecting referable diabetic retinopathy, vision-threatening diabetic retinopathy, possible glaucoma, and age-related macular degeneration (AMD) in community and clinic-based multiethnic populations with diabetes.Diagnostic performance of a DLS for diabetic retinopathy and related eye diseases was evaluated using 494 661 retinal images. A DLS was trained for detecting diabetic retinopathy (using 76 370 images), possible glaucoma (125 189 images), and AMD (72 610 images), and performance of DLS was evaluated for detecting diabetic retinopathy (using 112 648 images), possible glaucoma (71 896 images), and AMD (35 948 images). Training of the DLS was completed in May 2016, and validation of the DLS was completed in May 2017 for detection of referable diabetic retinopathy (moderate nonproliferative diabetic retinopathy or worse) and vision-threatening diabetic retinopathy (severe nonproliferative diabetic retinopathy or worse) using a primary validation data set in the Singapore National Diabetic Retinopathy Screening Program and 10 multiethnic cohorts with diabetes.Use of a deep learning system.Area under the receiver operating characteristic curve (AUC) and sensitivity and specificity of the DLS with professional graders (retinal specialists, general ophthalmologists, trained graders, or optometrists) as the reference standard.In the primary validation dataset (n = 14 880 patients; 71 896 images; mean [SD] age, 60.2 [2.2] years; 54.6\\% men), the prevalence of referable diabetic retinopathy was 3.0\\%; vision-threatening diabetic retinopathy, 0.6\\%; possible glaucoma, 0.1\\%; and AMD, 2.5\\%. The AUC of the DLS for referable diabetic retinopathy was 0.936 (95\\% CI, 0.925-0.943), sensitivity was 90.5\\% (95\\% CI, 87.3\\%-93.0\\%), and specificity was 91.6\\% (95\\% CI, 91.0\\%-92.2\\%). For vision-threatening diabetic retinopathy, AUC was 0.958 (95\\% CI, 0.956-0.961), sensitivity was 100\\% (95\\% CI, 94.1\\%-100.0\\%), and specificity was 91.1\\% (95\\% CI, 90.7\\%-91.4\\%). For possible glaucoma, AUC was 0.942 (95\\% CI, 0.929-0.954), sensitivity was 96.4\\% (95\\% CI, 81.7\\%-99.9\\%), and specificity was 87.2\\% (95\\% CI, 86.8\\%-87.5\\%). For AMD, AUC was 0.931 (95\\% CI, 0.928-0.935), sensitivity was 93.2\\% (95\\% CI, 91.1\\%-99.8\\%), and specificity was 88.7\\% (95\\% CI, 88.3\\%-89.0\\%). For referable diabetic retinopathy in the 10 additional datasets, AUC range was 0.889 to 0.983 (n = 40 752 images).In this evaluation of retinal images from multiethnic cohorts of patients with diabetes, the DLS had high sensitivity and specificity for identifying diabetic retinopathy and related eye diseases. Further research is necessary to evaluate the applicability of the DLS in health care settings and the utility of the DLS to improve vision outcomes.}},
  issn     = {0098-7484},
  doi      = {10.1001/jama.2017.18152},
  url      = {https://doi.org/10.1001/jama.2017.18152},
  eprint   = {https://jamanetwork.com/journals/jama/articlepdf/2665775/jama\_ting\_2017\_oi\_170140.pdf}
}

@article{Esteva2017DNNSkinCancer,
  doi       = {10.1038/nature21056},
  url       = {https://doi.org/10.1038/nature21056},
  year      = {2017},
  month     = jan,
  publisher = {Springer Science and Business Media {LLC}},
  volume    = {542},
  number    = {7639},
  pages     = {115--118},
  author    = {Andre Esteva and Brett Kuprel and Roberto A. Novoa and Justin Ko and Susan M. Swetter and Helen M. Blau and Sebastian Thrun},
  title     = {Dermatologist-level classification of skin cancer with deep neural networks},
  journal   = {Nature}
}

@article{Peng2019DeepSeeNet,
  title   = {DeepSeeNet: A Deep Learning Model for Automated Classification of Patient-based Age-related Macular Degeneration Severity from Color Fundus Photographs},
  journal = {Ophthalmology},
  volume  = {126},
  number  = {4},
  pages   = {565-575},
  year    = {2019},
  issn    = {0161-6420},
  doi     = {https://doi.org/10.1016/j.ophtha.2018.11.015},
  url     = {https://www.sciencedirect.com/science/article/pii/S0161642018321857},
  author  = {Yifan Peng and Shazia Dharssi and Qingyu Chen and Tiarnan D. Keenan and Elvira Agrón and Wai T. Wong and Emily Y. Chew and Zhiyong Lu}
}

@article{Rayner2015EvidenceCongnitiveFixation,
  title    = {Evidence for direct cognitive control of fixation durations during reading},
  journal  = {Current Opinion in Behavioral Sciences},
  volume   = {1},
  pages    = {107-112},
  year     = {2015},
  note     = {Cognitive control},
  issn     = {2352-1546},
  doi      = {https://doi.org/10.1016/j.cobeha.2014.10.008},
  url      = {https://www.sciencedirect.com/science/article/pii/S235215461400028X},
  author   = {Keith Rayner and Eyal M Reingold},
  abstract = {The control of eye movements during reading is discussed with emphasis on the direct cognitive-control hypothesis. According to this hypothesis, the durations of eye fixations are controlled on a moment-to-moment basis by cognitive processes associated with processing the lexical and linguistic properties of the fixated word. When the critical role of parafoveal lexical processing is taken into account, the tight timing constraints imposed by neural delays are not inconsistent with the direct cognitive control hypothesis. Several convergent lines of research using distributional analysis techniques and gaze-contingent display change paradigms are reviewed which provide strong support for the validity of the hypothesis. It is concluded that eye movements and gaze contingent display paradigms offer cognitive neuroscience promising avenues for understanding not only reading but on-line processing activities in a number of different tasks.}
}

@article{Anderson2004EyeMovementNegativeSupport,
  doi       = {10.1111/j.0956-7976.2004.00656.x},
  url       = {https://doi.org/10.1111/j.0956-7976.2004.00656.x},
  year      = {2004},
  month     = apr,
  publisher = {{SAGE} Publications},
  volume    = {15},
  number    = {4},
  pages     = {225--231},
  author    = {J. R. Anderson and D. Bothell and S. Douglass},
  title     = {Eye Movements Do Not Reflect Retrieval Processes: Limits of the Eye-Mind Hypothesis},
  journal   = {Psychological Science}
}

@article{Cater2020BestPracticeEyeTracking,
  title    = {Best practices in eye tracking research},
  journal  = {International Journal of Psychophysiology},
  volume   = {155},
  pages    = {49-62},
  year     = {2020},
  issn     = {0167-8760},
  doi      = {https://doi.org/10.1016/j.ijpsycho.2020.05.010},
  url      = {https://www.sciencedirect.com/science/article/pii/S0167876020301458},
  author   = {Benjamin T. Carter and Steven G. Luke},
  keywords = {Eye tracking, Eye movements, Best practices, Open Science, Pupillometry},
  abstract = {This guide describes best practices in using eye tracking technology for research in a variety of disciplines. A basic outline of the anatomy and physiology of the eyes and of eye movements is provided, along with a description of the sorts of research questions eye tracking can address. We then explain how eye tracking technology works and what sorts of data it generates, and provide guidance on how to select and use an eye tracker as well as selecting appropriate eye tracking measures. Challenges to the validity of eye tracking studies are described, along with recommendations for overcoming these challenges. We then outline correct reporting standards for eye tracking studies.}
}

@inproceedings{Guan2006ThinkAloud,
  author    = {Guan, Zhiwei and Lee, Shirley and Cuddihy, Elisabeth and Ramey, Judith},
  title     = {The Validity of the Stimulated Retrospective Think-Aloud Method as Measured by Eye Tracking},
  year      = {2006},
  isbn      = {1595933727},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/1124772.1124961},
  doi       = {10.1145/1124772.1124961},
  abstract  = {Retrospective Think aloud (RTA) is a usability method that collects the verbalization of a user's performance after the performance is over. There has been little work done to investigate the validity and reliability of RTA. This paper reports on an experiment investigating these issues with a form of the method called stimulated RTA. By comparing subjects' verbalizations with their eye movements, we support the validity and reliability of stimulated RTA: the method provides a valid account of what people attended to in completing tasks, it has a low risk of introducing fabrications, and its validity isn't affected by task complexity. More detailed analysis of RTA shows that it also provides additional information about user's inferences and strategies in completing tasks. The findings of this study provide valuable support for usability practitioners to use RTA and to trust the users' performance information collected by this method in a usability study.},
  booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
  pages     = {1253–1262},
  numpages  = {10},
  keywords  = {reliability, validity, verbalization, usability research, retrospective think aloud, eye tracking},
  location  = {Montr\'{e}al, Qu\'{e}bec, Canada},
  series    = {CHI '06}
}

@article{S2016Parkinson,
  title    = {Accuracy and re-test reliability of mobile eye-tracking in Parkinson's disease and older adults},
  journal  = {Medical Engineering & Physics},
  volume   = {38},
  number   = {3},
  pages    = {308-315},
  year     = {2016},
  issn     = {1350-4533},
  doi      = {https://doi.org/10.1016/j.medengphy.2015.12.001},
  url      = {https://www.sciencedirect.com/science/article/pii/S1350453315002805},
  author   = {S. Stuart and L. Alcock and A. Godfrey and S. Lord and L. Rochester and B. Galna},
  keywords = {Parkinson's disease, Mobile eye-tracking, Accuracy, Reliability, Saccades, Walking},
  abstract = {Mobile eye-tracking is important for understanding the role of vision during real-world tasks in older adults (OA) and people with Parkinson's disease (PD). However, accuracy and reliability of such devices have not been established in these populations. We used a novel protocol to quantify accuracy and reliability of a mobile eye-tracker in OA and PD. A mobile eye-tracker (Dikablis) measured the saccade amplitudes of 20 OA and 14 PD on two occasions. Participants made saccades between targets placed 5°, 10° and 15° apart. Impact of visual correction (glasses) on saccadic amplitude measurement was also investigated in 10 OA. Saccade amplitude accuracy (median bias) was −1.21° but a wide range of bias (−7.73° to 5.81°) was seen in OA and PD, with large vertical saccades (15°) being least accurate. Reliability assessment showed a median difference between sessions of <1° for both groups, with poor to good relative agreement (Spearman rho: 0.14 to 0.85). Greater accuracy and reliability was observed in people without visual correction. Saccade amplitude can be measured with variable accuracy and reliability using a mobile eye-tracker in OA and PD. Human, technological and study-specific protocol factors may introduce error and are discussed along with methodological recommendations.}
}


@article{Crawford2015Alzheimer,
  author   = {Crawford, Trevor},
  title    = {The disengagement of visual attention in Alzheimer's disease: a longitudinal eye-tracking study},
  journal  = {Frontiers in Aging Neuroscience},
  volume   = {7},
  pages    = {118},
  year     = {2015},
  url      = {https://www.frontiersin.org/article/10.3389/fnagi.2015.00118},
  doi      = {10.3389/fnagi.2015.00118},
  issn     = {1663-4365},
  abstract = {Introduction: Eye tracking provides a convenient and promising biological marker of cognitive impairment in patients with neurodegenerative disease. Here we report a longitudinal study of saccadic eye movements in a sample of patients with Alzheimer's disease and elderly control participants who were assessed at the start of the study and followed up 12-months later.Methods: Eye movements were measured in the standard gap and overlap paradigms, to examine the longitudinal trends in the ability to disengage attention from a visual target.Results: Overall patients with Alzheimer's disease had slower reaction times than the control group. However, after 12-months, both groups showed faster and comparable reductions in reaction times to the gap, compared to the overlap stimulus. Interestingly, there was a general improvement for both groups with more accurately directed saccades and speeding of reaction times after 12-months.Conclusions: These findings point to the value of longer-term studies and follow-up assessment to ascertain the effects of dementia on oculomotor control.}
}

@inbook{Levy2010Schizophrenia,
  author    = {Levy, Deborah L.
               and Sereno, Anne B.
               and Gooding, Diane C.
               and O'Driscoll, Gilllian A.},
  editor    = {Swerdlow, Neal R.},
  title     = {Eye Tracking Dysfunction in Schizophrenia: Characterization and Pathophysiology},
  booktitle = {Behavioral Neurobiology of Schizophrenia and Its Treatment},
  year      = {2010},
  publisher = {Springer Berlin Heidelberg},
  address   = {Berlin, Heidelberg},
  pages     = {311--347},
  abstract  = {Eye tracking dysfunction (ETD) is one of the most widely replicated behavioral deficits in schizophrenia and is over-represented in clinically unaffected first-degree relatives of schizophrenia patients. Here, we provide an overview of research relevant to the characterization and pathophysiology of this impairment. Deficits are most robust in the maintenance phase of pursuit, particularly during the tracking of predictable target movement. Impairments are also found in pursuit initiation and correlate with performance on tests of motion processing, implicating early sensory processing of motion signals. Taken together, the evidence suggests that ETD involves higher-order structures, including the frontal eye fields, which adjust the gain of the pursuit response to visual and anticipated target movement, as well as early parts of the pursuit pathway, including motion areas (the middle temporal area and the adjacent medial superior temporal area). Broader application of localizing behavioral paradigms in patient and family studies would be advantageous for refining the eye tracking phenotype for genetic studies.},
  isbn      = {978-3-642-13717-4},
  doi       = {10.1007/7854_2010_60},
  url       = {https://doi.org/10.1007/7854_2010_60}
}

@inbook{Belen2021Autism,
  author    = {de Belen, Ryan Anthony Jalova and Bednarz, Tomasz and Sowmya, Arcot},
  title     = {EyeXplain Autism: Interactive System for Eye Tracking Data Analysis and Deep Neural Network Interpretation for Autism Spectrum Disorder Diagnosis},
  year      = {2021},
  isbn      = {9781450380959},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3411763.3451784},
  abstract  = { Over the past decade, Deep Neural Networks (DNN) applied to eye tracking data have seen tremendous progress in their ability to perform Autism Spectrum Disorder (ASD) diagnosis. Despite their promising accuracy, DNNs are often seen as ’black boxes’ by physicians unfamiliar with the technology. In this paper, we present EyeXplain Autism, an interactive system that enables physicians to analyse eye tracking data, perform automated diagnosis and interpret DNN predictions. Here we discuss the design, development and sample scenario to illustrate the potential of our system to aid in ASD diagnosis. Unlike existing eye tracking software, our system combines traditional eye tracking visualisation and analysis tools with a data-driven knowledge to enhance medical decision-making for physicians.},
  booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
  articleno = {364},
  numpages  = {7}
}


@article{Reichle2012EyeMindLink1,
  abstract = {Nonreading tasks that share some (but not all) of the task demands of reading have often been used to make inferences about how cognition influences when the eyes move during reading. In this article, we use variants of the E-Z Reader model of eye-movement control in reading to simulate eye-movement behavior in several of these tasks, including z-string reading, target-word search, and visual search of Landolt Cs arranged in both linear and circular arrays. These simulations demonstrate that a single computational framework is sufficient to simulate eye movements in both reading and nonreading tasks but also suggest that there are task-specific differences in both saccadic targeting (i.e., decisions about where to move the eyes) and the coupling between saccadic programming and the movement of attention (i.e., decisions about when to move the eyes). These findings suggest that some aspects of the eye–mind link are flexible and can be configured in a manner that supports efficient task},
  author   = {Reichle, Erik D. and Pollatsek, Alexander and Rayner, Keith},
  issn     = {0033-295X},
  journal  = {Psychological Review},
  keywords = {E-Z Reader, attention, reading, saccades, visual search, Attention, Cognition, Comprehension, Computer Simulation, Eye Movement Measurements, Eye Movements, Fixation, Ocular, Humans, Models, Psychological, Reaction Time, Reading, Saccades, Vision, Ocular, Visual Acuity, Visual Perception, Eye Movements, Mind, Reading, Visual Search},
  number   = {1},
  pages    = {155 - 185},
  title    = {Using E-Z Reader to simulate eye movements in nonreading tasks: A unified framework for understanding the eye–mind link.},
  volume   = {119},
  url      = {https://search.ebscohost.com/login.aspx?direct=true&db=pdh&AN=2012-00103-003&site=ehost-live&scope=site},
  year     = {2012}
}

@article{Reichle2010EyeMindLink2,
  author   = {Erik D. Reichle and  Andrew E. Reineberg and Jonathan W. Schooler},
  title    = {Eye Movements During Mindless Reading},
  journal  = {Psychological Science},
  volume   = {21},
  number   = {9},
  pages    = {1300-1310},
  year     = {2010},
  doi      = {10.1177/0956797610378686},
  note     = {PMID: 20679524},
  url      = { 
              https://doi.org/10.1177/0956797610378686
              
              },
  eprint   = { 
              https://doi.org/10.1177/0956797610378686
              
              },
  abstract = { Mindless reading occurs when the eyes continue moving across the page even though the mind is thinking about something unrelated to the text. Despite how commonly it occurs, very little is known about mindless reading. The present experiment examined eye movements during mindless reading. Comparisons of fixation-duration measures collected during intervals of normal reading and intervals of mindless reading indicate that fixations during the latter were longer and less affected by lexical and linguistic variables than fixations during the former. Also, eye movements immediately preceding self-caught mind wandering were especially erratic. These results suggest that the cognitive processes that guide eye movements during normal reading are not engaged during mindless reading. We discuss the implications of these findings for theories of eye movement control in reading, for the distinction between experiential awareness and meta-awareness, and for reading comprehension. }
}


@inproceedings{Manning2003ExpreienceRadiologist,
  author       = {David Manning and Susan C. Ethell and Trevor Crawford},
  title        = {{Eye-tracking AFROC study of the influence of experience and training on chest x-ray interpretation}},
  volume       = {5034},
  booktitle    = {Medical Imaging 2003: Image Perception, Observer Performance, and Technology Assessment},
  editor       = {Dev P. Chakraborty and Elizabeth A. Krupinski},
  organization = {International Society for Optics and Photonics},
  publisher    = {SPIE},
  pages        = {257 -- 266},
  year         = {2003},
  doi          = {10.1117/12.479985},
  url          = {https://doi.org/10.1117/12.479985}
}

@article{Bruny2019DiagnosticInterpretation,
  doi       = {10.1186/s41235-019-0159-2},
  url       = {https://doi.org/10.1186/s41235-019-0159-2},
  year      = {2019},
  month     = feb,
  publisher = {Springer Science and Business Media {LLC}},
  volume    = {4},
  number    = {1},
  author    = {Tad T. Bruny{\'{e}} and Trafton Drew and Donald L. Weaver and Joann G. Elmore},
  title     = {A review of eye tracking for understanding and improving diagnostic interpretation},
  journal   = {Cognitive Research: Principles and Implications}
}

@article{Nicholas2015PassingGlance,
  title    = {A Passing Glance? Differences in Eye Tracking and Gaze Patterns Between Trainees and Experts Reading Plain Film Bunion Radiographs},
  journal  = {The Journal of Foot and Ankle Surgery},
  volume   = {54},
  number   = {3},
  pages    = {382-391},
  year     = {2015},
  issn     = {1067-2516},
  doi      = {https://doi.org/10.1053/j.jfas.2014.08.013},
  url      = {https://www.sciencedirect.com/science/article/pii/S1067251614004190},
  author   = {Nicholas A. Giovinco and Steven M. Sutton and John D. Miller and Timothy M. Rankin and Grant W. Gonzalez and Bijan Najafi and David G. Armstrong},
  keywords = {ergonomic, gaze, hallux valgus, radiograph, saccadic data, surgery},
  abstract = {Eye tracking and gaze pattern studies have been used to evaluate human behavior for decades. This is because of its ability to reveal conscious and subconscious behaviors when subjects are tasked with observation, decision making, and surgical performance. Many have popularized the use of this technology for radiographic assessment while evaluating radiologist behaviors, but little has been described for surgeon behavior patterns when evaluating preoperative deformities by radiograph. Because the radiographic assessment strongly influences surgical selection, the present study was designed to evaluate the differences between groups of novice and experienced surgeons' gaze patterns when tasked to describe hallux valgus deformities. The subjects were asked to rate the deformity as “none,” “mild,” “moderate,” or “severe.” Using an externally mounted eye tracking system, our study assessed saccades, fixations, overall time spent per radiograph, and the subjects' chosen bunion rating. Both the novice and advanced groups of foot and ankle surgeons were tasked to evaluate 25 total anteroposterior radiographs from patients who presented with a primary complaint of bunion pain. These patients were chosen at random, such that all participating surgeons had no previous patient familiarization. Statistically significant differences were observed with regard to the activity and rating of the moderate bunion films. The experience of surgeons does appear to modify gaze behavior with respect to time and attention, such that less overall time spent per image is needed by the advanced group, with improved efficiency. Future academic curriculum and training techniques could be developed to reflect these potential technical differences in search behavior, diagnostic technique, and surgical selection strategy.}
}

@inproceedings{Saab2021EyeTrackingCXRClassification,
  author    = {Saab, Khaled
               and Hooper, Sarah M.
               and Sohoni, Nimit S.
               and Parmar, Jupinder
               and Pogatchnik, Brian
               and Wu, Sen
               and Dunnmon, Jared A.
               and Zhang, Hongyang R.
               and Rubin, Daniel
               and R{\'e}, Christopher},
  editor    = {de Bruijne, Marleen
               and Cattin, Philippe C.
               and Cotin, St{\'e}phane
               and Padoy, Nicolas
               and Speidel, Stefanie
               and Zheng, Yefeng
               and Essert, Caroline},
  title     = {Observational Supervision for Medical Image Classification Using Gaze Data},
  booktitle = {Medical Image Computing and Computer Assisted Intervention -- MICCAI 2021},
  year      = {2021},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {603--614},
  abstract  = {Deep learning models have demonstrated favorable performance on many medical image classification tasks. However, they rely on expensive hand-labeled datasets that are time-consuming to create. In this work, we explore a new supervision source to training deep learning models by using gaze data that is passively and cheaply collected during a clinician's workflow. We focus on three medical imaging tasks, including classifying chest X-ray scans for pneumothorax and brain MRI slices for metastasis, two of which we curated gaze data for. The gaze data consists of a sequence of fixation locations on the image from an expert trying to identify an abnormality. Hence, the gaze data contains rich information about the image that can be used as a powerful supervision source. We first identify a set of gaze features and show that they indeed contain class-discriminative information. Then, we propose two methods for incorporating gaze features into deep learning pipelines. When no task labels are available, we combine multiple gaze features to extract weak labels and use them as the sole source of supervision (Gaze-WS). When task labels are available, we propose to use the gaze features as auxiliary task labels in a multi-task learning framework (Gaze-MTL). On three medical image classification tasks, our Gaze-WS method without task labels comes within 5 AUROC points (1.7 precision points) of models trained with task labels. With task labels, our Gaze-MTL method can improve performance by 2.4 AUROC points (4 precision points) over multiple baselines.},
  isbn      = {978-3-030-87196-3}
}



@article{Leslie2000CTClinicalData,
  doi       = {10.1259/bjr.73.874.11271897},
  url       = {https://doi.org/10.1259/bjr.73.874.11271897},
  year      = {2000},
  month     = oct,
  publisher = {British Institute of Radiology},
  volume    = {73},
  number    = {874},
  pages     = {1052--1055},
  author    = {A Leslie and A J Jones and P R Goddard},
  title     = {The influence of clinical information on the reporting of {CT} by radiologists.},
  journal   = {The British Journal of Radiology}
}


@inproceedings{Palinko2010EyeTrackingCognitiveLoad-1,
  author    = {Palinko, Oskar and Kun, Andrew L. and Shyrokov, Alexander and Heeman, Peter},
  title     = {Estimating Cognitive Load Using Remote Eye Tracking in a Driving Simulator},
  year      = {2010},
  isbn      = {9781605589947},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/1743666.1743701},
  doi       = {10.1145/1743666.1743701},
  abstract  = {We report on the results of a study in which pairs of subjects were involved in spoken dialogues and one of the subjects also operated a simulated vehicle. We estimated the driver's cognitive load based on pupil size measurements from a remote eye tracker. We compared the cognitive load estimates based on the physiological pupillometric data and driving performance data. The physiological and performance measures show high correspondence suggesting that remote eye tracking might provide reliable driver cognitive load estimation, especially in simulators. We also introduced a new pupillometric cognitive load measure that shows promise in tracking cognitive load changes on time scales of several seconds.},
  booktitle = {Proceedings of the 2010 Symposium on Eye-Tracking Research &amp; Applications},
  pages     = {141–144},
  numpages  = {4},
  keywords  = {cognitive load, pupillometry, eye tracking},
  location  = {Austin, Texas},
  series    = {ETRA '10}
}



@inproceedings{Zagermann2016EyeTrackingCognitiveLoad-2,
  author    = {Zagermann, Johannes and Pfeil, Ulrike and Reiterer, Harald},
  title     = {Measuring Cognitive Load Using Eye Tracking Technology in Visual Computing},
  year      = {2016},
  isbn      = {9781450348188},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2993901.2993908},
  doi       = {10.1145/2993901.2993908},
  abstract  = {In this position paper we encourage the use of eye tracking measurements to investigate users' cognitive load while interacting with a system. We start with an overview of how eye movements can be interpreted to provide insight about cognitive processes and present a descriptive model representing the relations of eye movements and cognitive load. Then, we discuss how specific characteristics of human-computer interaction (HCI) interfere with the model and impede the application of eye tracking data to measure cognitive load in visual computing. As a result, we present a refined model, embedding the characteristics of HCI into the relation of eye tracking data and cognitive load. Based on this, we argue that eye tracking should be considered as a valuable instrument to analyze cognitive processes in visual computing and suggest future research directions to tackle outstanding issues.},
  booktitle = {Proceedings of the Sixth Workshop on Beyond Time and Errors on Novel Evaluation Methods for Visualization},
  pages     = {78–85},
  numpages  = {8},
  keywords  = {cognitive load, novel evaluation methods, eye tracking},
  location  = {Baltimore, MD, USA},
  series    = {BELIV '16}
}


@article{Wang2014EyeTrackingCognitiveLoad-3,
  title    = {An eye-tracking study of website complexity from cognitive load perspective},
  journal  = {Decision Support Systems},
  volume   = {62},
  pages    = {1-10},
  year     = {2014},
  issn     = {0167-9236},
  doi      = {https://doi.org/10.1016/j.dss.2014.02.007},
  url      = {https://www.sciencedirect.com/science/article/pii/S0167923614000402},
  author   = {Qiuzhen Wang and Sa Yang and Manlu Liu and Zike Cao and Qingguo Ma},
  keywords = {HCI, Eye-tracking, Website complexity, Cognitive load, Task complexity, Online shopping},
  abstract = {Online shopping is becoming one of the most popular applications on the Internet. Websites are the important interfaces in HCI (Human–Computer Interaction). Website design significantly affects online shopping behavior. This research used eye-tracker to track the eye-movement process for 42 college students when they were surfing websites with different levels of complexity and completing simple and complex tasks respectively. The study examines how website complexity and task complexity jointly affect users' visual attention and behavior due to different cognitive loads. The study fills a research gap by examining this phenomenon from the cognitive load perspective and taking the moderate effect of task complexity into consideration. The results show that task complexity can moderate the effect of website complexity on users' visual attention and behavior. Specifically, when users conducted a simple task, fixation count and task completion time were at the highest level on the website with high complexity, while fixation duration was not significantly different on the websites with different complexity. However, when users conducted a complex task on a website with medium complexity, task completion time, fixation count, and fixation duration were all at their highest level. The load theory of attention was used to provide the explanation for the results. The findings provide guidelines for website managers and designers to maximize users' visual attention.}
}

@article{Krejtz2018EyeTrackingCognitiveLoad-4,
  doi       = {10.1371/journal.pone.0203629},
  author    = {Krejtz, Krzysztof AND Duchowski, Andrew T. AND Niedzielska, Anna AND Biele, Cezary AND Krejtz, Izabela},
  journal   = {PLOS ONE},
  publisher = {Public Library of Science},
  title     = {Eye tracking cognitive load using pupil diameter and microsaccades with fixed gaze},
  year      = {2018},
  month     = {09},
  volume    = {13},
  url       = {https://doi.org/10.1371/journal.pone.0203629},
  pages     = {1-23},
  abstract  = {Pupil diameter and microsaccades are captured by an eye tracker and compared for their suitability as indicators of cognitive load (as beset by task difficulty). Specifically, two metrics are tested in response to task difficulty: (1) the change in pupil diameter with respect to inter- or intra-trial baseline, and (2) the rate and magnitude of microsaccades. Participants performed easy and difficult mental arithmetic tasks while fixating a central target. Inter-trial change in pupil diameter and microsaccade magnitude appear to adequately discriminate task difficulty, and hence cognitive load, if the implied causality can be assumed. This paper’s contribution corroborates previous work concerning microsaccade magnitude and extends this work by directly comparing microsaccade metrics to pupillometric measures. To our knowledge this is the first study to compare the reliability and sensitivity of task-evoked pupillary and microsaccadic measures of cognitive load.},
  number    = {9}
}


@phdthesis{Klingner2010EyeTrackingCognitiveLoad-5,
  author   = {Klingner,Jeff},
  year     = {2010},
  title    = {Measuring Cognitive Load During Visual Tasks by Combining Pupillometry and Eye Tracking},
  journal  = {ProQuest Dissertations and Theses},
  pages    = {133},
  note     = {Copyright - Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works; Last updated - 2021-09-28},
  abstract = {Visualizations and visual interfaces can provide the means to analyze and communicate complex information, but such interfaces often overwhelm or confuse their users. Evaluating an interfaces's propensity to overload users requires the ability to assess cognitive load.Changes in cognitive load cause very small dilations of the pupils. In controlled settings, high-precision pupil measurements can be used to detect small differences in cognitive load at time scales shorter than one second. However, cognitive pupillometry has been generally limited to experiments using auditory stimuli and a blank visual field, because the pupils' responsiveness to changes in brightness and other visual details interferes with load-induced pupil dilations.In this dissertation, I present several improvements in methods for measuring cognitive load using pupillary dilations. First, I extend the set of eye tracking equipment validated for cognitive pupillometry, by determining the pupillometric precision of a remote-camera eye tracker and using remote camera equipment to replicate classic cognitive pupillometry experiments performed originally using head-mounted cameras. Second, I extend the applicability of cognitive pupillometry in visual tasks by developing fixation-aligned averaging methods to to handle the unpredictability of visual attention, and by demonstrating the measurement of cognitive load during visual search and map reading. I describe the methods used to accomplish these results, including experimental protocols and data processing methods to control or correct for various non-cognitive pupillary reflexes and methods for combining pupillometry with eye tracking. I present and discuss a new finding of a cognitive load advantage to visual presentation of simple arithmetic and memorization tasks.},
  keywords = {Changes in cognitive load; Pupillometric precision; Visual attention; Fixation-aligned averaging method; Neurosciences; Computer science; 0317:Neurosciences; 0984:Computer science},
  isbn     = {9798678103307},
  language = {English},
  url      = {https://gateway.library.qut.edu.au/login?url=https://www.proquest.com/dissertations-theses/measuring-cognitive-load-during-visual-tasks/docview/2449659813/se-2?accountid=13380}
}

@incollection{Chromik2021,
  title     = {Human-{{XAI Interaction}}: A {{Review}} and {{Design Principles}} for {{Explanation User Interfaces}}},
  booktitle = {Human-{{Computer Interaction}} \textendash{} {{INTERACT}} 2021},
  author    = {Chromik, Michael and Butz, Andreas},
  editor    = {Ardito, Carmelo and Lanzilotti, Rosa and Malizia, Alessio and Petrie, Helen and Piccinno, Antonio and Desolda, Giuseppe and Inkpen, Kori},
  year      = {2021}
}

%%% Start of the multimodal part %%%
@article{Wang2020DeepMultimodal,
  title   = {Deep multimodal fusion by channel exchanging},
  author  = {Wang, Yikai and Huang, Wenbing and Sun, Fuchun and Xu, Tingyang and Rong, Yu and Huang, Junzhou},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {33},
  year    = {2020}
}

@inproceedings{Park2017RDFNet,
  author    = {Park, Seong-Jin and Hong, Ki-Sang and Lee, Seungyong},
  title     = {RDFNet: RGB-D Multi-Level Residual Feature Fusion for Indoor Semantic Segmentation},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  month     = {Oct},
  year      = {2017}
}

@article{Valada2019MultimodalSemanticSegmentation,
  doi       = {10.1007/s11263-019-01188-y},
  url       = {https://doi.org/10.1007/s11263-019-01188-y},
  year      = {2019},
  month     = jul,
  publisher = {Springer Science and Business Media {LLC}},
  volume    = {128},
  number    = {5},
  pages     = {1239--1285},
  author    = {Abhinav Valada and Rohit Mohan and Wolfram Burgard},
  title     = {Self-Supervised Model Adaptation for Multimodal Semantic Segmentation},
  journal   = {International Journal of Computer Vision}
}

@inproceedings{Hu2019DMC,
  author    = {Hu, Di and Nie, Feiping and Li, Xuelong},
  title     = {Deep Multimodal Clustering for Unsupervised Audiovisual Learning},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2019}
}


@article{Tang2020MultiModalAutism,
  author         = {Tang, Michelle and Kumar, Pulkit and Chen, Hao and Shrivastava, Abhinav},
  title          = {Deep Multimodal Learning for the Diagnosis of Autism Spectrum Disorder},
  journal        = {Journal of Imaging},
  volume         = {6},
  year           = {2020},
  number         = {6},
  article-number = {47},
  url            = {https://www.mdpi.com/2313-433X/6/6/47},
  issn           = {2313-433X},
  abstract       = {Recent medical imaging technologies, specifically functional magnetic resonance imaging (fMRI), have advanced the diagnosis of neurological and neurodevelopmental disorders by allowing scientists and physicians to observe the activity within and between different regions of the brain. Deep learning methods have frequently been implemented to analyze images produced by such technologies and perform disease classification tasks; however, current state-of-the-art approaches do not take advantage of all the information offered by fMRI scans. In this paper, we propose a deep multimodal model that learns a joint representation from two types of connectomic data offered by fMRI scans. Incorporating two functional imaging modalities in an automated end-to-end autism diagnosis system will offer a more comprehensive picture of the neural activity, and thus allow for more accurate diagnoses. Our multimodal training strategy achieves a classification accuracy of 74% and a recall of 95%, as well as an F1 score of 0.805, and its overall performance is superior to using only one type of functional data.},
  doi            = {10.3390/jimaging6060047}
}


@inproceedings{Yang2017MutiModalTemporalData,
  author    = {Yang, Xitong and Ramesh, Palghat and Chitta, Radha and Madhvanath, Sriganesh and Bernal, Edgar A. and Luo, Jiebo},
  title     = {Deep Multimodal Representation Learning From Temporal Data},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {July},
  year      = {2017}
}

@inproceedings{Hu2019ScalableMultiModal,
  author    = {Hu, Peng and Zhen, Liangli and Peng, Dezhong and Liu, Pei},
  title     = {Scalable Deep Multimodal Learning for Cross-Modal Retrieval},
  year      = {2019},
  isbn      = {9781450361729},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi-org.ezp01.library.qut.edu.au/10.1145/3331184.3331213},
  doi       = {10.1145/3331184.3331213},
  abstract  = {Cross-modal retrieval takes one type of data as the query to retrieve relevant data of another type. Most of existing cross-modal retrieval approaches were proposed to learn a common subspace in a joint manner, where the data from all modalities have to be involved during the whole training process. For these approaches, the optimal parameters of different modality-specific transformations are dependent on each other and the whole model has to be retrained when handling samples from new modalities. In this paper, we present a novel cross-modal retrieval method, called Scalable Deep Multimodal Learning (SDML). It proposes to predefine a common subspace, in which the between-class variation is maximized while the within-class variation is minimized. Then, it trains m modality-specific networks for m modalities (one network for each modality) to transform the multimodal data into the predefined common subspace to achieve multimodal learning. Unlike many of the existing methods, our method can train different modality-specific networks independently and thus be scalable to the number of modalities. To the best of our knowledge, the proposed SDML could be one of the first works to independently project data of an unfixed number of modalities into a predefined common subspace. Comprehensive experimental results on four widely-used benchmark datasets demonstrate that the proposed method is effective and efficient in multimodal learning and outperforms the state-of-the-art methods in cross-modal retrieval.},
  booktitle = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages     = {635–644},
  numpages  = {10},
  keywords  = {multimodal learning, cross-modal retrieval, representation learning},
  location  = {Paris, France},
  series    = {SIGIR'19}
}

@inproceedings{Mroueh2015AudioVisualSpeechRecognitionMultiModal,
  author    = {Mroueh, Youssef and Marcheret, Etienne and Goel, Vaibhava},
  booktitle = {2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title     = {Deep multimodal learning for Audio-Visual Speech Recognition},
  year      = {2015},
  volume    = {},
  number    = {},
  pages     = {2130-2134},
  doi       = {10.1109/ICASSP.2015.7178347}
}

@article{Feng2021MultiModalAutonomousDriving,
  author  = {Feng, Di and Haase-Schütz, Christian and Rosenbaum, Lars and Hertlein, Heinz and Gläser, Claudius and Timm, Fabian and Wiesbeck, Werner and Dietmayer, Klaus},
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  title   = {Deep Multi-Modal Object Detection and Semantic Segmentation for Autonomous Driving: Datasets, Methods, and Challenges},
  year    = {2021},
  volume  = {22},
  number  = {3},
  pages   = {1341-1360},
  doi     = {10.1109/TITS.2020.2972974}
}


@article{Ramachandram2017DeepMultiModalSurvey,
  author  = {Ramachandram, Dhanesh and Taylor, Graham W.},
  journal = {IEEE Signal Processing Magazine},
  title   = {Deep Multimodal Learning: A Survey on Recent Advances and Trends},
  year    = {2017},
  volume  = {34},
  number  = {6},
  pages   = {96-108},
  doi     = {10.1109/MSP.2017.2738401}
}


@article{Lahat2015MultiModalDataFusion,
  author  = {Lahat, Dana and Adali, Tülay and Jutten, Christian},
  journal = {Proceedings of the IEEE},
  title   = {Multimodal Data Fusion: An Overview of Methods, Challenges, and Prospects},
  year    = {2015},
  volume  = {103},
  number  = {9},
  pages   = {1449-1477},
  doi     = {10.1109/JPROC.2015.2460697}
}

@inproceedings{Hori2017AttentionBasedFusion,
  author    = {Hori, Chiori and Hori, Takaaki and Lee, Teng-Yok and Zhang, Ziming and Harsham, Bret and Hershey, John R. and Marks, Tim K. and Sumi, Kazuhiko},
  title     = {Attention-Based Multimodal Fusion for Video Description},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  month     = {Oct},
  year      = {2017}
}

@article{Wang2020ChannelExchange,
  title   = {Deep multimodal fusion by channel exchanging},
  author  = {Wang, Yikai and Huang, Wenbing and Sun, Fuchun and Xu, Tingyang and Rong, Yu and Huang, Junzhou},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {33},
  year    = {2020}
}

@inproceedings{Vielzeuf2018CentralNet,
  author    = {Vielzeuf, Valentin and Lechervy, Alexis and Pateux, Stephane and Jurie, Frederic},
  title     = {CentralNet: a Multilayer Approach for Multimodal Fusion},
  booktitle = {Proceedings of the European Conference on Computer Vision (ECCV) Workshops},
  month     = {September},
  year      = {2018}
}

@article{Hevner2004DSRM,
  title     = {Design science in information systems research},
  author    = {Hevner, Alan R and March, Salvatore T and Park, Jinsoo and Ram, Sudha},
  journal   = {MIS quarterly},
  pages     = {75--105},
  year      = {2004},
  publisher = {JSTOR}
}


@article{Peffers2007DSRMForIS,
  author    = { Ken   Peffers  and  Tuure   Tuunanen  and  Marcus A.   Rothenberger  and  Samir   Chatterjee },
  title     = {A Design Science Research Methodology for Information Systems Research},
  journal   = {Journal of Management Information Systems},
  volume    = {24},
  number    = {3},
  pages     = {45-77},
  year      = {2007},
  publisher = {Routledge},
  doi       = {10.2753/MIS0742-1222240302},
  url       = {https://doi.org/10.2753/MIS0742-1222240302},
  eprint    = {https://doi.org/10.2753/MIS0742-1222240302}
}


@inproceedings{Sutskever2014Seq2Seq,
  title     = {Sequence to sequence learning with neural networks},
  author    = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  booktitle = {Advances in neural information processing systems},
  pages     = {3104--3112},
  year      = {2014}
}

@inproceedings{Vaswani2017Transformer,
  title     = {Attention is all you need},
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle = {Advances in neural information processing systems},
  pages     = {5998--6008},
  year      = {2017}
}


@inproceedings{Rodin2019MultimodalMIMICShortReport,
  author    = {Rodin, Ivan and Fedulova, Irina and Shelmanov, Artem and Dylov, Dmitry V.},
  booktitle = {2019 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)},
  title     = {Multitask and Multimodal Neural Network Model for Interpretable Analysis of X-ray Images},
  year      = {2019},
  volume    = {},
  number    = {},
  pages     = {1601-1604},
  doi       = {10.1109/BIBM47256.2019.8983272}
}


@article{McLaughlin2017EvaluateRadiologistPeformanceLevel,
  title={Computing eye gaze metrics for the automatic assessment of radiographer performance during X-ray image interpretation},
  author={McLaughlin, Laura and Bond, Raymond and Hughes, Ciara and McConnell, Jonathan and McFadden, Sonyia},
  journal={International journal of medical informatics},
  volume={105},
  pages={11--21},
  year={2017},
  publisher={Elsevier}
}