% Encoding: UTF-8

@article{Rudin2019LessonFromAICompetition,
  journal   = {Harvard Data Science Review},
  doi       = {10.1162/99608f92.5a8a3a3d},
  number    = {2},
  note      = {https://hdsr.mitpress.mit.edu/pub/f9kuryi8},
  publisher = {},
  title     = {Why Are We Using Black Box Models in AI When We Don’t Need To? A Lesson From An Explainable AI Competition},
  url       = {https://hdsr.mitpress.mit.edu/pub/f9kuryi8},
  volume    = {1},
  author    = {Rudin, Cynthia and Radin, Joanna},
  date      = {2019-11-22},
  year      = {2019},
  month     = {11},
  day       = {22}
}


@misc{EU2018GDPR,
  title  = {2018 reform of EU data protection rules},
  url    = {https://eur-lex.europa.eu/eli/reg/2016/679/2016-05-04},
  author = {{European Commission}},
  date   = {2016-04-27},
  year   = {2016},
  month  = {04}
}

@article{Rajpurkar2017CheXNet,
  author     = {Pranav Rajpurkar and
                Jeremy Irvin and
                Kaylie Zhu and
                Brandon Yang and
                Hershel Mehta and
                Tony Duan and
                Daisy Yi Ding and
                Aarti Bagul and
                Curtis P. Langlotz and
                Katie S. Shpanskaya and
                Matthew P. Lungren and
                Andrew Y. Ng},
  title      = {CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with
                Deep Learning},
  journal    = {CoRR},
  volume     = {abs/1711.05225},
  year       = {2017},
  url        = {http://arxiv.org/abs/1711.05225},
  eprinttype = {arXiv},
  eprint     = {1711.05225},
  timestamp  = {Fri, 26 Nov 2021 17:17:06 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1711-05225.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{Rajpurkar2018CheXNeXt,
  doi       = {10.1371/journal.pmed.1002686},
  author    = {Rajpurkar, Pranav AND Irvin, Jeremy AND Ball, Robyn L. AND Zhu, Kaylie AND Yang, Brandon AND Mehta, Hershel AND Duan, Tony AND Ding, Daisy AND Bagul, Aarti AND Langlotz, Curtis P. AND Patel, Bhavik N. AND Yeom, Kristen W. AND Shpanskaya, Katie AND Blankenberg, Francis G. AND Seekins, Jayne AND Amrhein, Timothy J. AND Mong, David A. AND Halabi, Safwan S. AND Zucker, Evan J. AND Ng, Andrew Y. AND Lungren, Matthew P.},
  journal   = {PLOS Medicine},
  publisher = {Public Library of Science},
  title     = {Deep learning for chest radiograph diagnosis: A retrospective comparison of the CheXNeXt algorithm to practicing radiologists},
  year      = {2018},
  month     = {11},
  volume    = {15},
  url       = {https://doi.org/10.1371/journal.pmed.1002686},
  pages     = {1-17},
  number    = {11}
}


@article{Waite2017RadiologistError,
  doi       = {10.2214/ajr.16.16963},
  url       = {https://doi.org/10.2214/ajr.16.16963},
  year      = {2017},
  month     = apr,
  publisher = {American Roentgen Ray Society},
  volume    = {208},
  number    = {4},
  pages     = {739--749},
  author    = {Stephen Waite and Jinel Scott and Brian Gale and Travis Fuchs and Srinivas Kolla and Deborah Reede},
  title     = {Interpretive Error in Radiology},
  journal   = {American Journal of Roentgenology}
}

@article{Sunshine2004RadiologyShortageUS,
  doi       = {10.2214/ajr.182.2.1820301},
  url       = {https://doi.org/10.2214/ajr.182.2.1820301},
  year      = {2004},
  month     = feb,
  publisher = {American Roentgen Ray Society},
  volume    = {182},
  number    = {2},
  pages     = {301--305},
  author    = {Jonathan H. Sunshine and C. Douglas Maynard and Joan Paros and Howard P. Forman},
  title     = {Update on the Diagnostic Radiologist Shortage},
  journal   = {American Journal of Roentgenology}
}


@article{Rimmer2017RadiologistShortageUk,
  doi       = {10.1136/bmj.j4683},
  url       = {https://doi.org/10.1136/bmj.j4683},
  year      = {2017},
  month     = oct,
  publisher = {{BMJ}},
  pages     = {j4683},
  author    = {Abi Rimmer},
  title     = {Radiologist shortage leaves patient care at risk,  warns royal college},
  journal   = {{BMJ}}
}


@inproceedings{Liu2019ReportGeneration,
  title     = {Clinically Accurate Chest X-Ray Report Generation},
  author    = {Liu, Guanxiong and Hsu, Tzu-Ming Harry and McDermott, Matthew and Boag, Willie and Weng, Wei-Hung and Szolovits, Peter and Ghassemi, Marzyeh},
  booktitle = {Proceedings of the 4th Machine Learning for Healthcare Conference},
  pages     = {249--269},
  year      = {2019},
  editor    = {Doshi-Velez, Finale and Fackler, Jim and Jung, Ken and Kale, David and Ranganath, Rajesh and Wallace, Byron and Wiens, Jenna},
  volume    = {106},
  series    = {Proceedings of Machine Learning Research},
  month     = {09--10 Aug},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v106/liu19a/liu19a.pdf},
  url       = {https://proceedings.mlr.press/v106/liu19a.html}
}

@inproceedings{SeyyedKalantari2020CheXclusion,
  doi       = {10.1142/9789811232701_0022},
  url       = {https://doi.org/10.1142/9789811232701_0022},
  year      = {2020},
  month     = nov,
  publisher = {{WORLD} {SCIENTIFIC}},
  author    = {Laleh Seyyed-Kalantari and Guanxiong Liu and Matthew McDermott and Irene Y. Chen and Marzyeh Ghassemi},
  title     = {{CheXclusion}: Fairness gaps in deep chest X-ray classifiers},
  booktitle = {Biocomputing 2021}
}


@article{Ribeiro2016LIME,
  author     = {Marco T{\'{u}}lio Ribeiro and
                Sameer Singh and
                Carlos Guestrin},
  title      = {"Why Should {I} Trust You?": Explaining the Predictions of Any Classifier},
  journal    = {CoRR},
  volume     = {abs/1602.04938},
  year       = {2016},
  url        = {http://arxiv.org/abs/1602.04938},
  eprinttype = {arXiv},
  eprint     = {1602.04938},
  timestamp  = {Mon, 13 Aug 2018 16:49:09 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/RibeiroSG16.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}


@article{Lundberg2017SHAP,
  author     = {Scott M. Lundberg and
                Su{-}In Lee},
  title      = {A unified approach to interpreting model predictions},
  journal    = {CoRR},
  volume     = {abs/1705.07874},
  year       = {2017},
  url        = {http://arxiv.org/abs/1705.07874},
  eprinttype = {arXiv},
  eprint     = {1705.07874},
  timestamp  = {Fri, 26 Nov 2021 16:33:36 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/LundbergL17.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{Moreira2021LINDA,
  doi       = {10.1016/j.dss.2021.113561},
  url       = {https://doi.org/10.1016/j.dss.2021.113561},
  year      = {2021},
  month     = nov,
  publisher = {Elsevier {BV}},
  volume    = {150},
  pages     = {113561},
  author    = {Catarina Moreira and Yu-Liang Chou and Mythreyi Velmurugan and Chun Ouyang and Renuka Sindhgatta and Peter Bruza},
  title     = {{LINDA}-{BN}: An interpretable probabilistic approach for demystifying black-box predictive models},
  journal   = {Decision Support Systems}
}

@article{Ahsan2020LIMEOnCXR,
  doi       = {10.3390/make2040027},
  url       = {https://doi.org/10.3390/make2040027},
  year      = {2020},
  month     = oct,
  publisher = {{MDPI} {AG}},
  volume    = {2},
  number    = {4},
  pages     = {490--504},
  author    = {Md Manjurul Ahsan and Kishor Datta Gupta and Mohammad Maminur Islam and Sajib Sen and Md. Lutfar Rahman and Mohammad Shakhawat Hossain},
  title     = {{COVID}-19 Symptoms Detection Based on {NasNetMobile} with Explainable {AI} Using Various Imaging Modalities},
  journal   = {Machine Learning and Knowledge Extraction}
}

@article{Teixeira2021LIMEAndGradCAMOnCXR,
  doi       = {10.3390/s21217116},
  url       = {https://doi.org/10.3390/s21217116},
  year      = {2021},
  month     = oct,
  publisher = {{MDPI} {AG}},
  volume    = {21},
  number    = {21},
  pages     = {7116},
  author    = {Lucas O. Teixeira and Rodolfo M. Pereira and Diego Bertolini and Luiz S. Oliveira and Loris Nanni and George D. C. Cavalcanti and Yandre M. G. Costa},
  title     = {Impact of Lung Segmentation on the Diagnosis and Explanation of {COVID}-19 in Chest X-ray Images},
  journal   = {Sensors}
}

@incollection{Zeiler2014UnderstandCNN,
  doi       = {10.1007/978-3-319-10590-1_53},
  url       = {https://doi.org/10.1007/978-3-319-10590-1_53},
  year      = {2014},
  publisher = {Springer International Publishing},
  pages     = {818--833},
  author    = {Matthew D. Zeiler and Rob Fergus},
  title     = {Visualizing and Understanding Convolutional Networks},
  booktitle = {Computer Vision {\textendash} {ECCV} 2014}
}

@inproceedings{Simonyan14DeepInside,
  author    = {Karen Simonyan and Andrea Vedaldi and Andrew Zisserman},
  title     = {Deep inside convolutional networks: Visualising image classification models and saliency maps},
  booktitle = {In Workshop at International Conference on Learning Representations},
  year      = {2014}
}

@article{Zhou2015CAM,
  author     = {Bolei Zhou and
                Aditya Khosla and
                {\`{A}}gata Lapedriza and
                Aude Oliva and
                Antonio Torralba},
  title      = {Learning Deep Features for Discriminative Localization},
  journal    = {CoRR},
  volume     = {abs/1512.04150},
  year       = {2015},
  url        = {http://arxiv.org/abs/1512.04150},
  eprinttype = {arXiv},
  eprint     = {1512.04150},
  timestamp  = {Mon, 13 Aug 2018 16:47:46 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/ZhouKLOT15.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Selvaraju2017GradCAM,
  author    = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  title     = {Grad-CAM: Visual Explanations From Deep Networks via Gradient-Based Localization},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  month     = {Oct},
  year      = {2017}
}

@inproceedings{Chattopadhay2018GradCAM++,
  doi       = {10.1109/wacv.2018.00097},
  url       = {https://doi.org/10.1109/wacv.2018.00097},
  year      = {2018},
  month     = mar,
  publisher = {{IEEE}},
  author    = {Aditya Chattopadhay and Anirban Sarkar and Prantik Howlader and Vineeth N Balasubramanian},
  title     = {Grad-{CAM}$\mathplus$$\mathplus$: Generalized Gradient-Based Visual Explanations for Deep Convolutional Networks},
  booktitle = {2018 {IEEE} Winter Conference on Applications of Computer Vision ({WACV})}
}

@article{Saporta2021BechmarkingSaliencyMethods,
  doi       = {10.1101/2021.02.28.21252634},
  url       = {https://doi.org/10.1101/2021.02.28.21252634},
  year      = {2021},
  month     = mar,
  publisher = {Cold Spring Harbor Laboratory},
  author    = {Adriel Saporta and Xiaotong Gui and Ashwin Agrawal and Anuj Pareek and Steven QH Truong and Chanh DT Nguyen and Van-Doan Ngo and Jayne Seekins and Francis G. Blankenberg and Andrew Y. Ng and Matthew P. Lungren and Pranav Rajpurkar},
  title     = {Benchmarking saliency methods for chest X-ray interpretation}
}

@article{Sundararajan2017IntegratedGradient,
  author     = {Mukund Sundararajan and
                Ankur Taly and
                Qiqi Yan},
  title      = {Axiomatic Attribution for Deep Networks},
  journal    = {CoRR},
  volume     = {abs/1703.01365},
  year       = {2017},
  url        = {http://arxiv.org/abs/1703.01365},
  eprinttype = {arXiv},
  eprint     = {1703.01365},
  timestamp  = {Mon, 13 Aug 2018 16:48:32 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/SundararajanTY17.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Ngiam2011MultiModalLearning,
  author    = {Jiquan Ngiam and Aditya Khosla and Mingyu Kim and Juhan Nam and Honglak Lee and Andrew Y. Ng},
  title     = {Multimodal Deep Learning},
  year      = {2011},
  cdate     = {1293840000000},
  pages     = {689-696},
  url       = {https://icml.cc/2011/papers/399_icmlpaper.pdf},
  booktitle = {ICML},
  crossref  = {conf/icml/2011}
}

@article{Karargyris2021EyeGazePaper,
  doi       = {10.1038/s41597-021-00863-5},
  url       = {https://doi.org/10.1038/s41597-021-00863-5},
  year      = {2021},
  month     = mar,
  publisher = {Springer Science and Business Media {LLC}},
  volume    = {8},
  number    = {1},
  author    = {Alexandros Karargyris and Satyananda Kashyap and Ismini Lourentzou and Joy T. Wu and Arjun Sharma and Matthew Tong and Shafiq Abedin and David Beymer and Vandana Mukherjee and Elizabeth A. Krupinski and Mehdi Moradi},
  title     = {Creation and validation of a chest X-ray dataset with eye-tracking and report dictation for {AI} development},
  journal   = {Scientific Data}
}


@misc{Johnson2021MIMIC_IV,
  doi       = {10.13026/S6N6-XD98},
  url       = {https://physionet.org/content/mimiciv/1.0/},
  author    = {Johnson,  Alistair and Bulgarelli,  Lucas and Pollard,  Tom and Horng,  Steven and Celi,  Leo Anthony and Mark,  Roger},
  title     = {MIMIC-IV},
  publisher = {PhysioNet},
  year      = {2021}
}

@misc{Johnson2021MIMIC_IV_ED,
  doi       = {10.13026/77Z6-9W59},
  url       = {https://physionet.org/content/mimic-iv-ed/1.0/},
  author    = {Johnson,  Alistair and Bulgarelli,  Lucas and Pollard,  Tom and Celi,  Leo Anthony and Mark,  Roger and Horng,  Steven},
  title     = {MIMIC-IV-ED},
  publisher = {PhysioNet},
  year      = {2021}
}

@misc{Johnson2019MIMIC_CXR,
  doi       = {10.13026/C2JT1Q},
  url       = {https://physionet.org/content/mimic-cxr/},
  author    = {Johnson,  Alistair E. W. and Pollard,  Tom and Mark,  Roger and Berkowitz,  Seth and Horng,  Steven},
  title     = {The MIMIC-CXR Database},
  publisher = {physionet.org},
  year      = {2019}
}

@article{DJohnson2019MIMIC_CXR_JPG,
  author     = {Alistair E. W. Johnson and
                Tom J. Pollard and
                Seth J. Berkowitz and
                Nathaniel R. Greenbaum and
                Matthew P. Lungren and
                Chih{-}ying Deng and
                Roger G. Mark and
                Steven Horng},
  title      = {{MIMIC-CXR-JPG:} {A} large publicly available database of labeled chest
                radiographs},
  journal    = {CoRR},
  volume     = {abs/1901.07042},
  year       = {2019},
  url        = {http://arxiv.org/abs/1901.07042},
  eprinttype = {arXiv},
  eprint     = {1901.07042},
  timestamp  = {Thu, 14 Oct 2021 09:14:23 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1901-07042.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}


@misc{Karargyris2020EyeGazeDataset,
  author    = {Karargyris, Alexandros and Kashyap, Satyananda and Lourentzou, Ismini and Wu, Joy and Tong, Matthew and Sharma, Arjun and Abedin, Shafiq and Beymer, David and Mukherjee, Vandana and Krupinski, Elizabeth and Moradi, Mehdi},
  booktitle = {Physionet},
  doi       = {https://doi.org/10.13026/qfdz-zr67},
  title     = {{Eye Gaze Data for Chest X-rays (version 1.0.0)}},
  url       = {https://physionet.org/content/egd-cxr/1.0.0/},
  year      = {2020}
}

@misc{Lanfredi2021REFLACX,
  doi       = {10.13026/E0DJ-8498},
  url       = {https://physionet.org/content/reflacx-xray-localization/1.0.0/},
  author    = {Bigolin Lanfredi,  Ricardo and Zhang,  Mingyuan and Auffermann,  William and Chan,  Jessica and Duong,  Phuong-Anh and Srikumar,  Vivek and Drew,  Trafton and Schroeder,  Joyce and Tasdizen,  Tolga},
  title     = {REFLACX: Reports and eye-tracking data for localization of abnormalities in chest x-rays},
  publisher = {PhysioNet},
  year      = {2021}
}

@article{LoyolaGonzalez2019BlackBoxVsWhiteBox,
  doi       = {10.1109/access.2019.2949286},
  url       = {https://doi.org/10.1109/access.2019.2949286},
  year      = {2019},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume    = {7},
  pages     = {154096--154113},
  author    = {Octavio Loyola-Gonzalez},
  title     = {Black-Box vs. White-Box: Understanding Their Advantages and Weaknesses From a Practical Point of View},
  journal   = {{IEEE} Access}
}

@misc{doshivelez2017RigorousInterpretable,
  title         = {Towards A Rigorous Science of Interpretable Machine Learning},
  author        = {Finale Doshi-Velez and Been Kim},
  year          = {2017},
  eprint        = {1702.08608},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML}
}

@article{Baltrušaitis2019MutimodalSurvey,
  author  = {Baltrušaitis, Tadas and Ahuja, Chaitanya and Morency, Louis-Philippe},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title   = {Multimodal Machine Learning: A Survey and Taxonomy},
  year    = {2019},
  volume  = {41},
  number  = {2},
  pages   = {423-443},
  doi     = {10.1109/TPAMI.2018.2798607}
}

@article{Ruder2017MultitaskLearning,
  author     = {Sebastian Ruder},
  title      = {An Overview of Multi-Task Learning in Deep Neural Networks},
  journal    = {CoRR},
  volume     = {abs/1706.05098},
  year       = {2017},
  url        = {http://arxiv.org/abs/1706.05098},
  eprinttype = {arXiv},
  eprint     = {1706.05098},
  timestamp  = {Mon, 13 Aug 2018 16:48:50 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/Ruder17a.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}


@article{Belle2020XAIPriciples,
  author     = {Vaishak Belle and
                Ioannis Papantonis},
  title      = {Principles and Practice of Explainable Machine Learning},
  journal    = {CoRR},
  volume     = {abs/2009.11698},
  year       = {2020},
  url        = {https://arxiv.org/abs/2009.11698},
  eprinttype = {arXiv},
  eprint     = {2009.11698},
  timestamp  = {Wed, 30 Sep 2020 16:16:22 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2009-11698.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Karlo2018XAISurvey,
  author    = {Došilović, Filip Karlo and Brčić, Mario and Hlupić, Nikica},
  booktitle = {2018 41st International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)},
  title     = {Explainable artificial intelligence: A survey},
  year      = {2018},
  volume    = {},
  number    = {},
  pages     = {0210-0215},
  doi       = {10.23919/MIPRO.2018.8400040}
}

@misc{Ribeiro2016Modelagnostic,
  title         = {Model-Agnostic Interpretability of Machine Learning},
  author        = {Marco Tulio Ribeiro and Sameer Singh and Carlos Guestrin},
  year          = {2016},
  eprint        = {1606.05386},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML}
}

@article{Ribeiro2018Anchors,
  title   = {Anchors: High-Precision Model-Agnostic Explanations},
  volume  = {32},
  url     = {https://ojs.aaai.org/index.php/AAAI/article/view/11491},
  number  = {1},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  author  = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year    = {2018},
  month   = {Apr.}
}

@inbook{Charnes1988ShapeleyValueInEconomic,
  author    = {Charnes, A.
               and Golany, B.
               and Keane, M.
               and Rousseau, J.},
  editor    = {Sengupta, Jati K.
               and Kadekodi, Gopal K.},
  title     = {Extremal Principle Solutions of Games in Characteristic Function Form: Core, Chebychev and Shapley Value Generalizations},
  booktitle = {Econometrics of Planning and Efficiency},
  year      = {1988},
  publisher = {Springer Netherlands},
  address   = {Dordrecht},
  pages     = {123--133},
  isbn      = {978-94-009-3677-5},
  doi       = {10.1007/978-94-009-3677-5_7},
  url       = {https://doi.org/10.1007/978-94-009-3677-5_7}
}



@inproceedings{Shrikumar2017DeepLIFT,
  title     = {Learning Important Features Through Propagating Activation Differences},
  author    = {Avanti Shrikumar and Peyton Greenside and Anshul Kundaje},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  pages     = {3145--3153},
  year      = {2017},
  editor    = {Precup, Doina and Teh, Yee Whye},
  volume    = {70},
  series    = {Proceedings of Machine Learning Research},
  month     = {06--11 Aug},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v70/shrikumar17a/shrikumar17a.pdf},
  url       = {https://proceedings.mlr.press/v70/shrikumar17a.html},
  abstract  = {The purported “black box” nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its `reference activation’ and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. Video tutorial: http://goo.gl/qKb7pL code: http://goo.gl/RM8jvH}
}


@article{Spinner2020explAIner,
  author  = {Spinner, Thilo and Schlegel, Udo and Schäfer, Hanna and El-Assady, Mennatallah},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  title   = {<bold>explAIner</bold>: A Visual Analytics Framework for Interactive and Explainable Machine Learning},
  year    = {2020},
  volume  = {26},
  number  = {1},
  pages   = {1064-1074},
  doi     = {10.1109/TVCG.2019.2934629}
}



@article{Singh2020ExplainableMedicalImage,
  author         = {Singh, Amitojdeep and Sengupta, Sourya and Lakshminarayanan, Vasudevan},
  title          = {Explainable Deep Learning Models in Medical Image Analysis},
  journal        = {Journal of Imaging},
  volume         = {6},
  year           = {2020},
  number         = {6},
  article-number = {52},
  url            = {https://www.mdpi.com/2313-433X/6/6/52},
  issn           = {2313-433X},
  abstract       = {Deep learning methods have been very effective for a variety of medical diagnostic tasks and have even outperformed human experts on some of those. However, the black-box nature of the algorithms has restricted their clinical use. Recent explainability studies aim to show the features that influence the decision of a model the most. The majority of literature reviews of this area have focused on taxonomy, ethics, and the need for explanations. A review of the current applications of explainable deep learning for different medical imaging tasks is presented here. The various approaches, challenges for clinical deployment, and the areas requiring further research are discussed here from a practical standpoint of a deep learning researcher designing a system for the clinical end-users.},
  doi            = {10.3390/jimaging6060052}
}


@article{Yuan2020DAM,
  author     = {Zhuoning Yuan and
                Yan Yan and
                Milan Sonka and
                Tianbao Yang},
  title      = {Robust Deep {AUC} Maximization: {A} New Surrogate Loss and Empirical
                Studies on Medical Image Classification},
  journal    = {CoRR},
  volume     = {abs/2012.03173},
  year       = {2020},
  url        = {https://arxiv.org/abs/2012.03173},
  eprinttype = {arXiv},
  eprint     = {2012.03173},
  timestamp  = {Wed, 09 Dec 2020 15:29:05 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2012-03173.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{Huang2005AUC,
  author  = {Jin Huang and Ling, C.X.},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  title   = {Using AUC and accuracy in evaluating learning algorithms},
  year    = {2005},
  volume  = {17},
  number  = {3},
  pages   = {299-310},
  doi     = {10.1109/TKDE.2005.50}
}

@article{Lobo2008AUDIsWrong,
  doi       = {10.1111/j.1466-8238.2007.00358.x},
  url       = {https://doi.org/10.1111/j.1466-8238.2007.00358.x},
  year      = {2008},
  month     = mar,
  publisher = {Wiley},
  volume    = {17},
  number    = {2},
  pages     = {145--151},
  author    = {Jorge M. Lobo and Alberto Jim{\'{e}}nez-Valverde and Raimundo Real},
  title     = {{AUC}: a misleading measure of the performance of predictive distribution models},
  journal   = {Global Ecology and Biogeography}
}

@article{HajianTilaki2013AUDOnMedical,
  title    = {Receiver Operating Characteristic ({ROC}) Curve Analysis for
              Medical Diagnostic Test Evaluation},
  author   = {Hajian-Tilaki, Karimollah},
  abstract = {This review provides the basic principle and rational for ROC
              analysis of rating and continuous diagnostic test results versus
              a gold standard. Derived indexes of accuracy, in particular area
              under the curve (AUC) has a meaningful interpretation for disease
              classification from healthy subjects. The methods of estimate of
              AUC and its testing in single diagnostic test and also
              comparative studies, the advantage of ROC curve to determine the
              optimal cut off values and the issues of bias and confounding
              have been discussed.},
  journal  = {Caspian J Intern Med},
  volume   = 4,
  number   = 2,
  pages    = {627--635},
  year     = 2013,
  keywords = {Area under the curve (AUC); Bias; Nonparametric; Parametric; ROC
              curve; Sensitivity; Specificity},
  language = {en}
}


%%% Need to include 

@article{Linardatos2021ExplainaleAIReview,
  author         = {Linardatos, Pantelis and Papastefanopoulos, Vasilis and Kotsiantis, Sotiris},
  title          = {Explainable AI: A Review of Machine Learning Interpretability Methods},
  journal        = {Entropy},
  volume         = {23},
  year           = {2021},
  number         = {1},
  article-number = {18},
  url            = {https://www.mdpi.com/1099-4300/23/1/18},
  pubmedid       = {33375658},
  issn           = {1099-4300},
  doi            = {10.3390/e23010018}
}

@article{Gao2012SSurrogatelossAUC,
  author     = {Wei Gao and
                Zhi{-}Hua Zhou},
  title      = {On the consistency of {AUC} Optimization},
  journal    = {CoRR},
  volume     = {abs/1208.0645},
  year       = {2012},
  url        = {http://arxiv.org/abs/1208.0645},
  eprinttype = {arXiv},
  eprint     = {1208.0645},
  timestamp  = {Mon, 13 Aug 2018 16:48:08 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1208-0645.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{Gao2013AUCSquareLoss,
  title     = {One-Pass AUC Optimization},
  author    = {Gao, Wei and Jin, Rong and Zhu, Shenghuo and Zhou, Zhi-Hua},
  booktitle = {Proceedings of the 30th International Conference on Machine Learning},
  pages     = {906--914},
  year      = {2013},
  editor    = {Dasgupta, Sanjoy and McAllester, David},
  volume    = {28},
  number    = {3},
  series    = {Proceedings of Machine Learning Research},
  address   = {Atlanta, Georgia, USA},
  month     = {17--19 Jun},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v28/gao13.pdf},
  url       = {https://proceedings.mlr.press/v28/gao13.html},
  abstract  = {AUC is an important performance measure and many algorithms have been devoted to AUC optimization, mostly by minimizing a surrogate convex loss on a training data set. In this work, we focus on one-pass AUC optimization that requires only going through the training data once without storing the entire training dataset, where conventional online learning algorithms cannot be applied directly because AUC is measured by a sum of losses defined over pairs of instances from different classes. We develop a regression-based algorithm which only needs to maintain the first and second order statistics of training data in memory, resulting a storage requirement independent from the size of training data. To efficiently handle high dimensional data, we develop a randomized algorithm that approximates the covariance matrices by low rank matrices. We verify, both theoretically and empirically, the effectiveness of the proposed algorithm.}
}

@inproceedings{Sulam2017MaximizingAUD,
  title     = {Maximizing AUC with Deep Learning for Classification of Imbalanced Mammogram Datasets.},
  author    = {Sulam, Jeremias and Ben-Ari, Rami and Kisilev, Pavel},
  booktitle = {VCBM},
  pages     = {131--135},
  year      = {2017}
}

@misc{Irvin2019Chexpert,
  title         = {CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison},
  author        = {Jeremy Irvin and Pranav Rajpurkar and Michael Ko and Yifan Yu and Silviana Ciurea-Ilcus and Chris Chute and Henrik Marklund and Behzad Haghgoo and Robyn Ball and Katie Shpanskaya and Jayne Seekins and David A. Mong and Safwan S. Halabi and Jesse K. Sandberg and Ricky Jones and David B. Larson and Curtis P. Langlotz and Bhavik N. Patel and Matthew P. Lungren and Andrew Y. Ng},
  year          = {2019},
  eprint        = {1901.07031},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@article{Huang2016DenseNet,
  author     = {Gao Huang and
                Zhuang Liu and
                Kilian Q. Weinberger},
  title      = {Densely Connected Convolutional Networks},
  journal    = {CoRR},
  volume     = {abs/1608.06993},
  year       = {2016},
  url        = {http://arxiv.org/abs/1608.06993},
  eprinttype = {arXiv},
  eprint     = {1608.06993},
  timestamp  = {Mon, 10 Sep 2018 15:49:32 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/HuangLW16a.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{He2015ResNet,
  author     = {Kaiming He and
                Xiangyu Zhang and
                Shaoqing Ren and
                Jian Sun},
  title      = {Deep Residual Learning for Image Recognition},
  journal    = {CoRR},
  volume     = {abs/1512.03385},
  year       = {2015},
  url        = {http://arxiv.org/abs/1512.03385},
  eprinttype = {arXiv},
  eprint     = {1512.03385},
  timestamp  = {Wed, 17 Apr 2019 17:23:45 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/HeZRS15.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}


@article{Cohen2020Covid19,
  title    = {Predicting {COVID-19} Pneumonia Severity on Chest X-ray With Deep
              Learning},
  author   = {Cohen, Joseph Paul and Dao, Lan and Roth, Karsten and Morrison,
              Paul and Bengio, Yoshua and Abbasi, Almas F and Shen, Beiyi and
              Mahsa, Hoshmand Kochi and Ghassemi, Marzyeh and Li, Haifang and
              Duong, Tim Q},
  abstract = {Introduction The need to streamline patient management for
              coronavirus disease-19 (COVID-19) has become more pressing than
              ever. Chest X-rays (CXRs) provide a non-invasive (potentially
              bedside) tool to monitor the progression of the disease. In this
              study, we present a severity score prediction model for COVID-19
              pneumonia for frontal chest X-ray images. Such a tool can gauge
              the severity of COVID-19 lung infections (and pneumonia in
              general) that can be used for escalation or de-escalation of care
              as well as monitoring treatment efficacy, especially in the ICU.
              Methods Images from a public COVID-19 database were scored
              retrospectively by three blinded experts in terms of the extent
              of lung involvement as well as the degree of opacity. A neural
              network model that was pre-trained on large (non-COVID-19) chest
              X-ray datasets is used to construct features for COVID-19 images
              which are predictive for our task. Results This study finds that
              training a regression model on a subset of the outputs from this
              pre-trained chest X-ray model predicts our geographic extent
              score (range 0-8) with 1.14 mean absolute error (MAE) and our
              lung opacity score (range 0-6) with 0.78 MAE. Conclusions These
              results indicate that our model's ability to gauge the severity
              of COVID-19 lung infections could be used for escalation or
              de-escalation of care as well as monitoring treatment efficacy,
              especially in the ICU. To enable follow up work, we make our
              code, labels, and data available online.},
  journal  = {Cureus},
  volume   = 12,
  number   = 7,
  pages    = {e9448},
  month    = jul,
  year     = 2020,
  keywords = {chest x-ray; covid-19 pneumonia; deep learning artificial
              intelligence; severity scoring},
  language = {en}
}

@inproceedings{Deng2009ImageNet,
  author    = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
  booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
  title     = {ImageNet: A large-scale hierarchical image database},
  year      = {2009},
  volume    = {},
  number    = {},
  pages     = {248-255},
  doi       = {10.1109/CVPR.2009.5206848}
}

@article{Dunnmon2019CNNOnCXRAssessment,
  doi       = {10.1148/radiol.2018181422},
  url       = {https://doi.org/10.1148/radiol.2018181422},
  year      = {2019},
  month     = feb,
  publisher = {Radiological Society of North America ({RSNA})},
  volume    = {290},
  number    = {2},
  pages     = {537--544},
  author    = {Jared A. Dunnmon and Darvin Yi and Curtis P. Langlotz and Christopher R{\'{e}} and Daniel L. Rubin and Matthew P. Lungren},
  title     = {Assessment of Convolutional Neural Networks for Automated Classification of Chest Radiographs},
  journal   = {Radiology}
}

@inproceedings{Alex2012AlexNet,
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {ImageNet Classification with Deep Convolutional Neural Networks},
  url       = {https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
  volume    = {25},
  year      = {2012}
}

@article{Zhuang2019TransferLearningSurvey,
  author     = {Fuzhen Zhuang and
                Zhiyuan Qi and
                Keyu Duan and
                Dongbo Xi and
                Yongchun Zhu and
                Hengshu Zhu and
                Hui Xiong and
                Qing He},
  title      = {A Comprehensive Survey on Transfer Learning},
  journal    = {CoRR},
  volume     = {abs/1911.02685},
  year       = {2019},
  url        = {http://arxiv.org/abs/1911.02685},
  eprinttype = {arXiv},
  eprint     = {1911.02685},
  timestamp  = {Sat, 29 Aug 2020 18:19:14 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1911-02685.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{Lin2014MSCOCO,
  author     = {Tsung{-}Yi Lin and
                Michael Maire and
                Serge J. Belongie and
                Lubomir D. Bourdev and
                Ross B. Girshick and
                James Hays and
                Pietro Perona and
                Deva Ramanan and
                Piotr Doll{\'{a}}r and
                C. Lawrence Zitnick},
  title      = {Microsoft {COCO:} Common Objects in Context},
  journal    = {CoRR},
  volume     = {abs/1405.0312},
  year       = {2014},
  url        = {http://arxiv.org/abs/1405.0312},
  eprinttype = {arXiv},
  eprint     = {1405.0312},
  timestamp  = {Mon, 13 Aug 2018 16:48:13 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/LinMBHPRDZ14.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{Kuznetsova2020OpenImages,
  author  = {Alina Kuznetsova and Hassan Rom and Neil Alldrin and Jasper Uijlings and Ivan Krasin and Jordi Pont-Tuset and Shahab Kamali and Stefan Popov and Matteo Malloci and Alexander Kolesnikov and Tom Duerig and Vittorio Ferrari},
  title   = {The Open Images Dataset V4: Unified image classification, object detection, and visual relationship detection at scale},
  year    = {2020},
  journal = {IJCV}
}


@article{Brunese2020ExplainableCovid19,
  title    = {Explainable Deep Learning for Pulmonary Disease and Coronavirus COVID-19 Detection from X-rays},
  journal  = {Computer Methods and Programs in Biomedicine},
  volume   = {196},
  pages    = {105608},
  year     = {2020},
  issn     = {0169-2607},
  doi      = {https://doi.org/10.1016/j.cmpb.2020.105608},
  url      = {https://www.sciencedirect.com/science/article/pii/S0169260720314413},
  author   = {Luca Brunese and Francesco Mercaldo and Alfonso Reginelli and Antonella Santone},
  keywords = {Coronavirus, COVID-19, Chest, Deep learning, Transfer learning, Artificial intelligence},
  abstract = {Background and Objective: Coronavirus disease (COVID-19) is an infectious disease caused by a new virus never identified before in humans. This virus causes respiratory disease (for instance, flu) with symptoms such as cough, fever and, in severe cases, pneumonia. The test to detect the presence of this virus in humans is performed on sputum or blood samples and the outcome is generally available within a few hours or, at most, days. Analysing biomedical imaging the patient shows signs of pneumonia. In this paper, with the aim of providing a fully automatic and faster diagnosis, we propose the adoption of deep learning for COVID-19 detection from X-rays. Method: In particular, we propose an approach composed by three phases: the first one to detect if in a chest X-ray there is the presence of a pneumonia. The second one to discern between COVID-19 and pneumonia. The last step is aimed to localise the areas in the X-ray symptomatic of the COVID-19 presence. Results and Conclusion: Experimental analysis on 6,523 chest X-rays belonging to different institutions demonstrated the effectiveness of the proposed approach, with an average time for COVID-19 detection of approximately 2.5 seconds and an average accuracy equal to 0.97.}
}

@article{Hou2021ExplainableCovid19,
  doi       = {10.1038/s41598-021-95680-6},
  url       = {https://doi.org/10.1038/s41598-021-95680-6},
  year      = {2021},
  month     = aug,
  publisher = {Springer Science and Business Media {LLC}},
  volume    = {11},
  number    = {1},
  author    = {Jie Hou and Terry Gao},
  title     = {Explainable {DCNN} based chest X-ray image analysis and classification for {COVID}-19 pneumonia detection},
  journal   = {Scientific Reports}
}


@article{Harezlak2018EyeTrackingInMedicine,
  title    = {Application of eye tracking in medicine: A survey, research issues and challenges},
  journal  = {Computerized Medical Imaging and Graphics},
  volume   = {65},
  pages    = {176-190},
  year     = {2018},
  note     = {Advances in Biomedical Image Processing},
  issn     = {0895-6111},
  doi      = {https://doi.org/10.1016/j.compmedimag.2017.04.006},
  url      = {https://www.sciencedirect.com/science/article/pii/S0895611117300435},
  author   = {Katarzyna Harezlak and Pawel Kasprowski},
  keywords = {Eye tracking medicine gaze},
  abstract = {The performance and quality of medical procedures and treatments are inextricably linked to technological development. The application of more advanced techniques provides the opportunity to gain wider knowledge and deeper understanding of the human body and mind functioning. The eye tracking methods used to register eye movement to find the direction and targets of a person's gaze are well in line with the nature of the topic. By providing methods for capturing and processing images of the eye it has become possible not only to reveal abnormalities in eye functioning but also to conduct cognitive studies focused on learning about peoples’ emotions and intentions. The usefulness of the application of eye tracking technology in medicine was proved in many research studies. The aim of this paper is to give an insight into those studies and the way they utilize eye imaging in medical applications. These studies were differentiated taking their purpose and experimental paradigms into account. Additionally, methods for eye movement visualization and metrics for its quantifying were presented. Apart from presenting the state of the art, the aim of the paper was also to point out possible applications of eye tracking in medicine that have not been exhaustively investigated yet, and are going to be a perspective long-term direction of research.}
}

@article{Castillo2020ClinicalInformationOnRadiology,
  doi       = {10.1002/jmrs.424},
  url       = {https://doi.org/10.1002/jmrs.424},
  year      = {2020},
  month     = sep,
  publisher = {Wiley},
  volume    = {68},
  number    = {1},
  pages     = {60--74},
  author    = {Chelsea Castillo and Tom Steffens and Lawrence Sim and Liam Caffery},
  title     = {The effect of clinical information on radiology reporting: A systematic review},
  journal   = {Journal of Medical Radiation Sciences}
}


@inproceedings{Blascheck2014VisualisingEyeTracking,
  title     = {State-of-the-Art of Visualization for Eye Tracking Data.},
  author    = {Blascheck, Tanja and Kurzhals, Kuno and Raschke, Michael and Burch, Michael and Weiskopf, Daniel and Ertl, Thomas},
  booktitle = {EuroVis (STARs)},
  year      = {2014}
}

@article{Cooke2005EyeTrackingUsability,
  author   = {Cooke,Lynne},
  year     = {2005},
  month    = {11},
  title    = {Eye Tracking: How It Works and How It Relates to Usability},
  journal  = {Technical Communication},
  volume   = {52},
  number   = {4},
  pages    = {456-463},
  note     = {Name - Association for Computing Machinery; Copyright - Copyright Society for Technical Communication Nov 2005; Document feature - ; Last updated - 2019-11-23; CODEN - TLCMBT; SubjectsTermNotLitGenreText - New York},
  abstract = {Cooke investigates how eye tracking works and how it relates to usability. Suggested areas for future eye-tracking research are also discussed.},
  keywords = {Printing; Digital electronics; Eyes & eyesight; Tracking control systems; Eye movements; New York},
  isbn     = {00493155},
  language = {English},
  url      = {https://gateway.library.qut.edu.au/login?url=https://www.proquest.com/scholarly-journals/eye-tracking-how-works-relates-usability/docview/220995671/se-2?accountid=13380}
}

@article{Fitts2005Cockpit,
  title     = {Eye movements of aircraft pilots during instrument-landing approaches},
  author    = {Fitts, Paul M and Jones, Richard E and Milton, John L},
  journal   = {Ergonomics: Psychological mechanisms and models in ergonomics},
  volume    = {3},
  pages     = {56},
  year      = {2005},
  publisher = {Taylor \& Francis}
}

@incollection{Schall2014EyeTrackingIntro,
  title     = {Introduction to Eye Tracking},
  editor    = {Jennifer {Romano Bergstrom} and Andrew Jonathan Schall},
  booktitle = {Eye Tracking in User Experience Design},
  publisher = {Morgan Kaufmann},
  address   = {Boston},
  pages     = {3-26},
  year      = {2014},
  isbn      = {978-0-12-408138-3},
  doi       = {https://doi.org/10.1016/B978-0-12-408138-3.00001-7},
  url       = {https://www.sciencedirect.com/science/article/pii/B9780124081383000017},
  author    = {Andrew Schall and Jennifer {Romano Bergstrom}},
  keywords  = {eye tracker, eye tracking, history of eye tracking, user experience, user research, UX, vision science},
  abstract  = {This chapter contains a brief history of eye tracking and how it has become a valuable methodology for user experience researchers. Readers will gain a basic understanding of how eye trackers can track the location of a user’s eye gaze and common visualizations used to analyze the eye-tracking data output.}
}

@article{Yates2018RedDotCXR,
  title    = {Machine learning “red dot”: open-source, cloud, deep convolutional neural networks in chest radiograph binary normality classification},
  journal  = {Clinical Radiology},
  volume   = {73},
  number   = {9},
  pages    = {827-831},
  year     = {2018},
  issn     = {0009-9260},
  doi      = {https://doi.org/10.1016/j.crad.2018.05.015},
  url      = {https://www.sciencedirect.com/science/article/pii/S000992601830206X},
  author   = {E.J. Yates and L.C. Yates and H. Harvey},
  abstract = {Aim
              To develop a machine learning-based model for the binary classification of chest radiography abnormalities, to serve as a retrospective tool in guiding clinician reporting prioritisation.
              Materials and methods
              The open-source machine learning library, Tensorflow, was used to retrain a final layer of the deep convolutional neural network, Inception, to perform binary normality classification on two, anonymised, public image datasets. Re-training was performed on 47,644 images using commodity hardware, with validation testing on 5,505 previously unseen radiographs. Confusion matrix analysis was performed to derive diagnostic utility metrics.
              Results
              A final model accuracy of 94.6% (95% confidence interval [CI]: 94.3–94.7%) based on an unseen testing subset (n=5,505) was obtained, yielding a sensitivity of 94.6% (95% CI: 94.4–94.7%) and a specificity of 93.4% (95% CI: 87.2–96.9%) with a positive predictive value (PPV) of 99.8% (95% CI: 99.7–99.9%) and area under the curve (AUC) of 0.98 (95% CI: 0.97–0.99).
              Conclusion
              This study demonstrates the application of a machine learning-based approach to classify chest radiographs as normal or abnormal. Its application to real-world datasets may be warranted in optimising clinician workload.}
}

@article{Szegedy2015InceptionV3,
  author     = {Christian Szegedy and
                Vincent Vanhoucke and
                Sergey Ioffe and
                Jonathon Shlens and
                Zbigniew Wojna},
  title      = {Rethinking the Inception Architecture for Computer Vision},
  journal    = {CoRR},
  volume     = {abs/1512.00567},
  year       = {2015},
  url        = {http://arxiv.org/abs/1512.00567},
  eprinttype = {arXiv},
  eprint     = {1512.00567},
  timestamp  = {Mon, 13 Aug 2018 16:49:07 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/SzegedyVISW15.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{Tang2020AbnormalityCXRCNN,
  doi       = {10.1038/s41746-020-0273-z},
  url       = {https://doi.org/10.1038/s41746-020-0273-z},
  year      = {2020},
  month     = may,
  publisher = {Springer Science and Business Media {LLC}},
  volume    = {3},
  number    = {1},
  author    = {Yu-Xing Tang and You-Bao Tang and Yifan Peng and Ke Yan and Mohammadhadi Bagheri and Bernadette A. Redd and Catherine J. Brandon and Zhiyong Lu and Mei Han and Jing Xiao and Ronald M. Summers},
  title     = {Automated abnormality classification of chest radiographs using deep convolutional neural networks},
  journal   = {npj Digital Medicine}
}

@article{Ting2017DLOnEyeDisease,
  author   = {Ting, Daniel Shu Wei and Cheung, Carol Yim-Lui and Lim, Gilbert and Tan, Gavin Siew Wei and Quang, Nguyen D. and Gan, Alfred and Hamzah, Haslina and Garcia-Franco, Renata and San Yeo, Ian Yew and Lee, Shu Yen and Wong, Edmund Yick Mun and Sabanayagam, Charumathi and Baskaran, Mani and Ibrahim, Farah and Tan, Ngiap Chuan and Finkelstein, Eric A. and Lamoureux, Ecosse L. and Wong, Ian Y. and Bressler, Neil M. and Sivaprasad, Sobha and Varma, Rohit and Jonas, Jost B. and He, Ming Guang and Cheng, Ching-Yu and Cheung, Gemmy Chui Ming and Aung, Tin and Hsu, Wynne and Lee, Mong Li and Wong, Tien Yin},
  title    = {{Development and Validation of a Deep Learning System for Diabetic Retinopathy and Related Eye Diseases Using Retinal Images From Multiethnic Populations With Diabetes}},
  journal  = {JAMA},
  volume   = {318},
  number   = {22},
  pages    = {2211-2223},
  year     = {2017},
  month    = {12},
  abstract = {{A deep learning system (DLS) is a machine learning technology with potential for screening diabetic retinopathy and related eye diseases.To evaluate the performance of a DLS in detecting referable diabetic retinopathy, vision-threatening diabetic retinopathy, possible glaucoma, and age-related macular degeneration (AMD) in community and clinic-based multiethnic populations with diabetes.Diagnostic performance of a DLS for diabetic retinopathy and related eye diseases was evaluated using 494 661 retinal images. A DLS was trained for detecting diabetic retinopathy (using 76 370 images), possible glaucoma (125 189 images), and AMD (72 610 images), and performance of DLS was evaluated for detecting diabetic retinopathy (using 112 648 images), possible glaucoma (71 896 images), and AMD (35 948 images). Training of the DLS was completed in May 2016, and validation of the DLS was completed in May 2017 for detection of referable diabetic retinopathy (moderate nonproliferative diabetic retinopathy or worse) and vision-threatening diabetic retinopathy (severe nonproliferative diabetic retinopathy or worse) using a primary validation data set in the Singapore National Diabetic Retinopathy Screening Program and 10 multiethnic cohorts with diabetes.Use of a deep learning system.Area under the receiver operating characteristic curve (AUC) and sensitivity and specificity of the DLS with professional graders (retinal specialists, general ophthalmologists, trained graders, or optometrists) as the reference standard.In the primary validation dataset (n = 14 880 patients; 71 896 images; mean [SD] age, 60.2 [2.2] years; 54.6\\% men), the prevalence of referable diabetic retinopathy was 3.0\\%; vision-threatening diabetic retinopathy, 0.6\\%; possible glaucoma, 0.1\\%; and AMD, 2.5\\%. The AUC of the DLS for referable diabetic retinopathy was 0.936 (95\\% CI, 0.925-0.943), sensitivity was 90.5\\% (95\\% CI, 87.3\\%-93.0\\%), and specificity was 91.6\\% (95\\% CI, 91.0\\%-92.2\\%). For vision-threatening diabetic retinopathy, AUC was 0.958 (95\\% CI, 0.956-0.961), sensitivity was 100\\% (95\\% CI, 94.1\\%-100.0\\%), and specificity was 91.1\\% (95\\% CI, 90.7\\%-91.4\\%). For possible glaucoma, AUC was 0.942 (95\\% CI, 0.929-0.954), sensitivity was 96.4\\% (95\\% CI, 81.7\\%-99.9\\%), and specificity was 87.2\\% (95\\% CI, 86.8\\%-87.5\\%). For AMD, AUC was 0.931 (95\\% CI, 0.928-0.935), sensitivity was 93.2\\% (95\\% CI, 91.1\\%-99.8\\%), and specificity was 88.7\\% (95\\% CI, 88.3\\%-89.0\\%). For referable diabetic retinopathy in the 10 additional datasets, AUC range was 0.889 to 0.983 (n = 40 752 images).In this evaluation of retinal images from multiethnic cohorts of patients with diabetes, the DLS had high sensitivity and specificity for identifying diabetic retinopathy and related eye diseases. Further research is necessary to evaluate the applicability of the DLS in health care settings and the utility of the DLS to improve vision outcomes.}},
  issn     = {0098-7484},
  doi      = {10.1001/jama.2017.18152},
  url      = {https://doi.org/10.1001/jama.2017.18152},
  eprint   = {https://jamanetwork.com/journals/jama/articlepdf/2665775/jama\_ting\_2017\_oi\_170140.pdf}
}

@article{Esteva2017DNNSkinCancer,
  doi       = {10.1038/nature21056},
  url       = {https://doi.org/10.1038/nature21056},
  year      = {2017},
  month     = jan,
  publisher = {Springer Science and Business Media {LLC}},
  volume    = {542},
  number    = {7639},
  pages     = {115--118},
  author    = {Andre Esteva and Brett Kuprel and Roberto A. Novoa and Justin Ko and Susan M. Swetter and Helen M. Blau and Sebastian Thrun},
  title     = {Dermatologist-level classification of skin cancer with deep neural networks},
  journal   = {Nature}
}

@article{Peng2019DeepSeeNet,
  title   = {DeepSeeNet: A Deep Learning Model for Automated Classification of Patient-based Age-related Macular Degeneration Severity from Color Fundus Photographs},
  journal = {Ophthalmology},
  volume  = {126},
  number  = {4},
  pages   = {565-575},
  year    = {2019},
  issn    = {0161-6420},
  doi     = {https://doi.org/10.1016/j.ophtha.2018.11.015},
  url     = {https://www.sciencedirect.com/science/article/pii/S0161642018321857},
  author  = {Yifan Peng and Shazia Dharssi and Qingyu Chen and Tiarnan D. Keenan and Elvira Agrón and Wai T. Wong and Emily Y. Chew and Zhiyong Lu}
}

@article{Rayner2015EvidenceCongnitiveFixation,
  title    = {Evidence for direct cognitive control of fixation durations during reading},
  journal  = {Current Opinion in Behavioral Sciences},
  volume   = {1},
  pages    = {107-112},
  year     = {2015},
  note     = {Cognitive control},
  issn     = {2352-1546},
  doi      = {https://doi.org/10.1016/j.cobeha.2014.10.008},
  url      = {https://www.sciencedirect.com/science/article/pii/S235215461400028X},
  author   = {Keith Rayner and Eyal M Reingold},
  abstract = {The control of eye movements during reading is discussed with emphasis on the direct cognitive-control hypothesis. According to this hypothesis, the durations of eye fixations are controlled on a moment-to-moment basis by cognitive processes associated with processing the lexical and linguistic properties of the fixated word. When the critical role of parafoveal lexical processing is taken into account, the tight timing constraints imposed by neural delays are not inconsistent with the direct cognitive control hypothesis. Several convergent lines of research using distributional analysis techniques and gaze-contingent display change paradigms are reviewed which provide strong support for the validity of the hypothesis. It is concluded that eye movements and gaze contingent display paradigms offer cognitive neuroscience promising avenues for understanding not only reading but on-line processing activities in a number of different tasks.}
}

@article{Anderson2004EyeMovementNegativeSupport,
  doi       = {10.1111/j.0956-7976.2004.00656.x},
  url       = {https://doi.org/10.1111/j.0956-7976.2004.00656.x},
  year      = {2004},
  month     = apr,
  publisher = {{SAGE} Publications},
  volume    = {15},
  number    = {4},
  pages     = {225--231},
  author    = {J. R. Anderson and D. Bothell and S. Douglass},
  title     = {Eye Movements Do Not Reflect Retrieval Processes: Limits of the Eye-Mind Hypothesis},
  journal   = {Psychological Science}
}

@article{Cater2020BestPracticeEyeTracking,
  title    = {Best practices in eye tracking research},
  journal  = {International Journal of Psychophysiology},
  volume   = {155},
  pages    = {49-62},
  year     = {2020},
  issn     = {0167-8760},
  doi      = {https://doi.org/10.1016/j.ijpsycho.2020.05.010},
  url      = {https://www.sciencedirect.com/science/article/pii/S0167876020301458},
  author   = {Benjamin T. Carter and Steven G. Luke},
  keywords = {Eye tracking, Eye movements, Best practices, Open Science, Pupillometry},
  abstract = {This guide describes best practices in using eye tracking technology for research in a variety of disciplines. A basic outline of the anatomy and physiology of the eyes and of eye movements is provided, along with a description of the sorts of research questions eye tracking can address. We then explain how eye tracking technology works and what sorts of data it generates, and provide guidance on how to select and use an eye tracker as well as selecting appropriate eye tracking measures. Challenges to the validity of eye tracking studies are described, along with recommendations for overcoming these challenges. We then outline correct reporting standards for eye tracking studies.}
}

@inproceedings{Guan2006ThinkAloud,
  author    = {Guan, Zhiwei and Lee, Shirley and Cuddihy, Elisabeth and Ramey, Judith},
  title     = {The Validity of the Stimulated Retrospective Think-Aloud Method as Measured by Eye Tracking},
  year      = {2006},
  isbn      = {1595933727},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/1124772.1124961},
  doi       = {10.1145/1124772.1124961},
  abstract  = {Retrospective Think aloud (RTA) is a usability method that collects the verbalization of a user's performance after the performance is over. There has been little work done to investigate the validity and reliability of RTA. This paper reports on an experiment investigating these issues with a form of the method called stimulated RTA. By comparing subjects' verbalizations with their eye movements, we support the validity and reliability of stimulated RTA: the method provides a valid account of what people attended to in completing tasks, it has a low risk of introducing fabrications, and its validity isn't affected by task complexity. More detailed analysis of RTA shows that it also provides additional information about user's inferences and strategies in completing tasks. The findings of this study provide valuable support for usability practitioners to use RTA and to trust the users' performance information collected by this method in a usability study.},
  booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
  pages     = {1253–1262},
  numpages  = {10},
  keywords  = {reliability, validity, verbalization, usability research, retrospective think aloud, eye tracking},
  location  = {Montr\'{e}al, Qu\'{e}bec, Canada},
  series    = {CHI '06}
}

@article{S2016Parkinson,
  title    = {Accuracy and re-test reliability of mobile eye-tracking in Parkinson's disease and older adults},
  journal  = {Medical Engineering & Physics},
  volume   = {38},
  number   = {3},
  pages    = {308-315},
  year     = {2016},
  issn     = {1350-4533},
  doi      = {https://doi.org/10.1016/j.medengphy.2015.12.001},
  url      = {https://www.sciencedirect.com/science/article/pii/S1350453315002805},
  author   = {S. Stuart and L. Alcock and A. Godfrey and S. Lord and L. Rochester and B. Galna},
  keywords = {Parkinson's disease, Mobile eye-tracking, Accuracy, Reliability, Saccades, Walking},
  abstract = {Mobile eye-tracking is important for understanding the role of vision during real-world tasks in older adults (OA) and people with Parkinson's disease (PD). However, accuracy and reliability of such devices have not been established in these populations. We used a novel protocol to quantify accuracy and reliability of a mobile eye-tracker in OA and PD. A mobile eye-tracker (Dikablis) measured the saccade amplitudes of 20 OA and 14 PD on two occasions. Participants made saccades between targets placed 5°, 10° and 15° apart. Impact of visual correction (glasses) on saccadic amplitude measurement was also investigated in 10 OA. Saccade amplitude accuracy (median bias) was −1.21° but a wide range of bias (−7.73° to 5.81°) was seen in OA and PD, with large vertical saccades (15°) being least accurate. Reliability assessment showed a median difference between sessions of <1° for both groups, with poor to good relative agreement (Spearman rho: 0.14 to 0.85). Greater accuracy and reliability was observed in people without visual correction. Saccade amplitude can be measured with variable accuracy and reliability using a mobile eye-tracker in OA and PD. Human, technological and study-specific protocol factors may introduce error and are discussed along with methodological recommendations.}
}


@article{Crawford2015Alzheimer,
  author   = {Crawford, Trevor},
  title    = {The disengagement of visual attention in Alzheimer's disease: a longitudinal eye-tracking study},
  journal  = {Frontiers in Aging Neuroscience},
  volume   = {7},
  pages    = {118},
  year     = {2015},
  url      = {https://www.frontiersin.org/article/10.3389/fnagi.2015.00118},
  doi      = {10.3389/fnagi.2015.00118},
  issn     = {1663-4365},
  abstract = {Introduction: Eye tracking provides a convenient and promising biological marker of cognitive impairment in patients with neurodegenerative disease. Here we report a longitudinal study of saccadic eye movements in a sample of patients with Alzheimer's disease and elderly control participants who were assessed at the start of the study and followed up 12-months later.Methods: Eye movements were measured in the standard gap and overlap paradigms, to examine the longitudinal trends in the ability to disengage attention from a visual target.Results: Overall patients with Alzheimer's disease had slower reaction times than the control group. However, after 12-months, both groups showed faster and comparable reductions in reaction times to the gap, compared to the overlap stimulus. Interestingly, there was a general improvement for both groups with more accurately directed saccades and speeding of reaction times after 12-months.Conclusions: These findings point to the value of longer-term studies and follow-up assessment to ascertain the effects of dementia on oculomotor control.}
}

@inbook{Levy2010Schizophrenia,
  author    = {Levy, Deborah L.
               and Sereno, Anne B.
               and Gooding, Diane C.
               and O'Driscoll, Gilllian A.},
  editor    = {Swerdlow, Neal R.},
  title     = {Eye Tracking Dysfunction in Schizophrenia: Characterization and Pathophysiology},
  booktitle = {Behavioral Neurobiology of Schizophrenia and Its Treatment},
  year      = {2010},
  publisher = {Springer Berlin Heidelberg},
  address   = {Berlin, Heidelberg},
  pages     = {311--347},
  abstract  = {Eye tracking dysfunction (ETD) is one of the most widely replicated behavioral deficits in schizophrenia and is over-represented in clinically unaffected first-degree relatives of schizophrenia patients. Here, we provide an overview of research relevant to the characterization and pathophysiology of this impairment. Deficits are most robust in the maintenance phase of pursuit, particularly during the tracking of predictable target movement. Impairments are also found in pursuit initiation and correlate with performance on tests of motion processing, implicating early sensory processing of motion signals. Taken together, the evidence suggests that ETD involves higher-order structures, including the frontal eye fields, which adjust the gain of the pursuit response to visual and anticipated target movement, as well as early parts of the pursuit pathway, including motion areas (the middle temporal area and the adjacent medial superior temporal area). Broader application of localizing behavioral paradigms in patient and family studies would be advantageous for refining the eye tracking phenotype for genetic studies.},
  isbn      = {978-3-642-13717-4},
  doi       = {10.1007/7854_2010_60},
  url       = {https://doi.org/10.1007/7854_2010_60}
}

@inbook{Belen2021Autism,
  author    = {de Belen, Ryan Anthony Jalova and Bednarz, Tomasz and Sowmya, Arcot},
  title     = {EyeXplain Autism: Interactive System for Eye Tracking Data Analysis and Deep Neural Network Interpretation for Autism Spectrum Disorder Diagnosis},
  year      = {2021},
  isbn      = {9781450380959},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3411763.3451784},
  abstract  = { Over the past decade, Deep Neural Networks (DNN) applied to eye tracking data have seen tremendous progress in their ability to perform Autism Spectrum Disorder (ASD) diagnosis. Despite their promising accuracy, DNNs are often seen as ’black boxes’ by physicians unfamiliar with the technology. In this paper, we present EyeXplain Autism, an interactive system that enables physicians to analyse eye tracking data, perform automated diagnosis and interpret DNN predictions. Here we discuss the design, development and sample scenario to illustrate the potential of our system to aid in ASD diagnosis. Unlike existing eye tracking software, our system combines traditional eye tracking visualisation and analysis tools with a data-driven knowledge to enhance medical decision-making for physicians.},
  booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
  articleno = {364},
  numpages  = {7}
}


@article{Reichle2012EyeMindLink1,
  abstract = {Nonreading tasks that share some (but not all) of the task demands of reading have often been used to make inferences about how cognition influences when the eyes move during reading. In this article, we use variants of the E-Z Reader model of eye-movement control in reading to simulate eye-movement behavior in several of these tasks, including z-string reading, target-word search, and visual search of Landolt Cs arranged in both linear and circular arrays. These simulations demonstrate that a single computational framework is sufficient to simulate eye movements in both reading and nonreading tasks but also suggest that there are task-specific differences in both saccadic targeting (i.e., decisions about where to move the eyes) and the coupling between saccadic programming and the movement of attention (i.e., decisions about when to move the eyes). These findings suggest that some aspects of the eye–mind link are flexible and can be configured in a manner that supports efficient task},
  author   = {Reichle, Erik D. and Pollatsek, Alexander and Rayner, Keith},
  issn     = {0033-295X},
  journal  = {Psychological Review},
  keywords = {E-Z Reader, attention, reading, saccades, visual search, Attention, Cognition, Comprehension, Computer Simulation, Eye Movement Measurements, Eye Movements, Fixation, Ocular, Humans, Models, Psychological, Reaction Time, Reading, Saccades, Vision, Ocular, Visual Acuity, Visual Perception, Eye Movements, Mind, Reading, Visual Search},
  number   = {1},
  pages    = {155 - 185},
  title    = {Using E-Z Reader to simulate eye movements in nonreading tasks: A unified framework for understanding the eye–mind link.},
  volume   = {119},
  url      = {https://search.ebscohost.com/login.aspx?direct=true&db=pdh&AN=2012-00103-003&site=ehost-live&scope=site},
  year     = {2012}
}

@article{Reichle2010EyeMindLink2,
  author   = {Erik D. Reichle and  Andrew E. Reineberg and Jonathan W. Schooler},
  title    = {Eye Movements During Mindless Reading},
  journal  = {Psychological Science},
  volume   = {21},
  number   = {9},
  pages    = {1300-1310},
  year     = {2010},
  doi      = {10.1177/0956797610378686},
  note     = {PMID: 20679524},
  url      = { 
              https://doi.org/10.1177/0956797610378686
              
              },
  eprint   = { 
              https://doi.org/10.1177/0956797610378686
              
              },
  abstract = { Mindless reading occurs when the eyes continue moving across the page even though the mind is thinking about something unrelated to the text. Despite how commonly it occurs, very little is known about mindless reading. The present experiment examined eye movements during mindless reading. Comparisons of fixation-duration measures collected during intervals of normal reading and intervals of mindless reading indicate that fixations during the latter were longer and less affected by lexical and linguistic variables than fixations during the former. Also, eye movements immediately preceding self-caught mind wandering were especially erratic. These results suggest that the cognitive processes that guide eye movements during normal reading are not engaged during mindless reading. We discuss the implications of these findings for theories of eye movement control in reading, for the distinction between experiential awareness and meta-awareness, and for reading comprehension. }
}


@inproceedings{Manning2003ExpreienceRadiologist,
  author       = {David Manning and Susan C. Ethell and Trevor Crawford},
  title        = {{Eye-tracking AFROC study of the influence of experience and training on chest x-ray interpretation}},
  volume       = {5034},
  booktitle    = {Medical Imaging 2003: Image Perception, Observer Performance, and Technology Assessment},
  editor       = {Dev P. Chakraborty and Elizabeth A. Krupinski},
  organization = {International Society for Optics and Photonics},
  publisher    = {SPIE},
  pages        = {257 -- 266},
  year         = {2003},
  doi          = {10.1117/12.479985},
  url          = {https://doi.org/10.1117/12.479985}
}


% Need to inlcude 
