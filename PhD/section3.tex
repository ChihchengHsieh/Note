\section{BACKGROUND AND LITERATURE REVIEW}

\subsection{Introductory Statement}

% context - provide some context to orient those readers who are less familiar with your topic and to establish the importance of your work.

% concepts - core concepts/notions that are required for the reader to know in order to understand your work

% need/motivation - state the need for your work, as an opposition between what the scientific community currently has and what it wants.

% position - positioning the research to carry out

% your contributions

%%%%%%%%%%%%%%%%%%% From reading Kenny's stage 2 %%%%%%%%%%%%%%%%%%%
% 3.1 Introductory Statement

% ?: more studies about the Automated chest radiograph interpretation.
% * A lot of information about datasets in \citet{SeyyedKalantari2020CheXclusion}

%  

%%%%%% Labeler %%%%%%
% Compared to other dataset, the CXR often come with the report from radiologists
% rather than a clear label. In favor of training machine learning algorithm, we
% need a labeler to extract class labels from report text.

%%%%%% Datasets %%%%%%
% 1. MIMIC CHR, 371,920 chest x-rays associated with 227,943 imaging studies
% sourced from the Beth Israel Deaconess Medical Center between 2011 - 2016.
% 2. PadChest
% 3. Open-I
% 4. CheXpert (224,316 chest radiographs of 65,240 patients) from Stanford
% Hospital, erformed between October 2002 and July 2017 in both inpatient and
% outpatient centers, along with their associated radiology reports.

%%%%%% Additional dataset %%%%%%
% REFLACX
% EyeGaze



% p.1 General talk about AI.
% ! Need to put some reference here.
With the outstanding performance of deep learning model, Artificial intelligent (AI) system have began to be adopted and impact various industries, including transportation, manufacturing, advertising, financial and medical industries. Using deep learning not only provides a high performance outcome, it also frees people from monotonous and complicated feature engineering works. The non-necessity of feature engineering makes it a perfect choice on handling unstructured data, such as image and text, which has been proven with astonishing results. \\


% p.2 Why the medical AI is needed.
One practical field adopting AI approaches is chest X-ray (CXR) interpretation. When patients is asked to take a X-ray, a still chest X-ray image will be developed for the radiologists to identify the diseases and injuries. Reading the CXR-image can be time-consuming and painstaking. Even the expert radiologists can have diagnosis error caused by fatigue or extrinsic distractions \citep{Waite2017RadiologistError}. Moreover, in many areas over the world, the shortage of radiologists occurs even in developed countries \citep{Sunshine2004RadiologyShortageUS} \citep{Rimmer2017RadiologistShortageUk}. A serious shortage of radiologists can increase the already heavy burden of workload for radiologists, which exacerbates the concern of fatigue mentioned above. An interactive AI-system can assist the radiologists to conduct more efficient interpretation by providing the professional radiologist with pre-identified suspicious areas and lesions. After the abnormalities have been located, radiologists can take less effort to only work on confirming the predictions and finalising the report. \\

% p.3 current implementation of deep learning on medical.
Since the deep learning is well known for processing image data. Enormous amount of studies have been explored for automated diagnosis conducted by AI-based predictive systems. \citet{Rajpurkar2017CheXNet} proposed CheXNet, which consists of 121-layer of Convolution Neural Network (CNN) to detect pneumonia by Chest X-ray images. CheXNet achieved 0.425 on F1 score, which outperforms practicing radiologist average of 0.387. Furthermore, a report text can also generated by only providing CXR image. \citet{Liu2019ReportGeneration} cooperate the CNN encoder, RNN decoder and reinforcement learning to generate the clinical report with human-level quality. With the works mentioned, the deep learning approach has been proven to generate valuable results. Those results can then be helpful for radiologists when they're interpreting Chest X-ray images. \\

% p.4 The need of XAI.
The deep neural network hold the nature of non-linearity and large amount of parameters, which allows it to fit complex functions. However, since the non-linearity in its activation functions, current neural networks can't provide its own insight of decision making process to human leading to the concern of reliability. In some cases, a wrong decision can be extremely expensive. For example, an incorrect decision on buy/sell stocks can cause the company to loss million dollars on the market. And, when a medical AI system provide a faulty prescription, it may result in an irreversible damage on patients. \\

As we known the serious impact on inaccurate prediction of model, governments and organisations have announced policies to manage and regulate the use of AI system. These policies protect the right of data subject who can be affected significantly by a the prediction of AI models. A well known example is the article 22 in General Data Protection Regulation (GDPR) states the end-user has the right to not involve in a decision, which only made by an AI-system without human intervention. Furthermore, the article 13 to 15 require the controller to provide meaningful information about the logic involved when the data is collected from the user \citep{EU2018GDPR}. \\

With the raise of concerns and regulations, a new strategy has to be developed to conquer the disadvantages of deep neural network and fulfill the requirements of provisions. Moreover, if the AI system want to be adopted and deployed broadly, relieving  the concern of untrustworthiness will be necessary. People is more prone to believe the prediction of black-box models if guaranteed that the output is traceable and understandable. A NIPS conference hold at 2018 conducted a thought experiments, which asks audience to decide which surgeon they would prefer to operate a life-saving surgery for them. The first option was a human surgeon who could explain the details of the surgery for you when had 85\% success rate. On the other side, the second option was a robot who could achieve 98\% success rate but no explanations would be provided and no question would be answer by the robot. At the end, the robot only received 1 vote among hundreds of audience from different domains \citep{Rudin2019LessonFromAICompetition}. The above example highlights the importance of explanation in reliability. Considering the issues mentioned above, we need the approach of explainable artificial intelligence (XAI) to convince the end-user to trust the AI system. \\

% p.5 Current XAI approach and some cases on medical or not medical image data. (One common strategy in research of computer vision is the saliency map.)
Explainable artificial intelligence (XAI) is a set of approaches to extract the insight of decision making process from black-box model. The retrieved insight provides meaningful information for users to comprehend how a specific output from AI system is obtained. Currently, multiple approaches has been developed to explain the computational decision made by black-box models. And, the approaches of XAI have been divided to two categories. The first one is to use \textit{transparent} models, which can be contemplated by human at once. However, using transparent model often associated with low performance on complex datasets. Another XAI approach, called post-hoc explaination, is to interpret opaque models after-the-fact. And the advatage of adopting post-hoc method is the interpretation for opaque models, which allow us to obtain both explainability and performance. Therefore, most of the researches target post-hoc explanation. Some common post-hoc approach are being used broadly, such as as LIME \citep{Ribeiro2016LIME}, SHAP \citep{Lundberg2017SHAP} and LINDA \citep{Moreira2021LINDA}. \citet{Ahsan2020LIMEOnCXR} and \citet{Teixeira2021LIMEAndGradCAMOnCXR} use LIME to interpret the results gained from the neural network. In addition, since the research on visualising trained CNN features \citep{Zeiler2014UnderstandCNN} \citep{Simonyan14DeepInside}, a set of activation-based and gradient-based localisation approaches has been developed and frequently used with image data for visualisation purpose, including CAM \citep{Zhou2015CAM}, \citep{Selvaraju2017GradCAM}, GradCAM++ \citep{Chattopadhay2018GradCAM++} and Integrated Gradient \citep{Sundararajan2017IntegratedGradient}. These gradient-based explaination methods are popular on processing image data because the heatmaps generated by these algorithms can overlap on original images to visualise the sailent pixels clearly. \\


% p.6 The gap of current XAI method. (different user group.)
In the work of CheXNet \citep{Rajpurkar2017CheXNet} and CheXNeXt \citep{Rajpurkar2018CheXNeXt}, Class Activation Mappings (CAMs) \citep{Zhou2015CAM} is used for generating heatmaps to show important features during classification. Then they observe that CAMs can successfully localise the pathologies it identified. However, this approach of interpretation can be inconsistent. \citet{Saporta2021BechmarkingSaliencyMethods} conducted a human benchmark to evaluate current saliency methods, which includes GradCAM, GradCAM ++ and Integrated Gradient. The evaluation result shows the saliency methods perform significantly worse than professional radiologists. Therefore, before applying medical AI system on the real cases, multiple issues have to be solved or mitigated to ensure the reliability. \\

% p.7 How would I like to close the gap.
To close the gap of current XAI methods on chest X-ray image, we want to apply multi-modal learning \citep{Ngiam2011MultiModalLearning}.  \citet{Castillo2020ClinicalInformationOnRadiology} found the clinical information communicated to the radiologist can improve interpretation accuracy, clinical relevance and reporting confidence. Since the positive impact of clinical information on radiology report, it should be inlcuded as a input of model to facilitate the decision making process. Instead only providing CXR images to the model, other modalities, such as eye tracking data, report text, bounding boxes and clinical data can also be included, as shown in Table \ref{tab:modalities_table}. In this work, we will mainly use the data from 6 different datasets. MIMIC-IV \citep{Johnson2021MIMIC_IV} and MIMIC-ED \citep{Johnson2021MIMIC_IV_ED} provide the clinical data about the patients. MIMIC-CXR \citep{Johnson2019MIMIC_CXR} and MIMIC-CXR-JPG \citep{DJohnson2019MIMIC_CXR_JPG} contain the CXR images and the report text from the hospital where patients took CXR . Eye gaze data \citep{Karargyris2020EyeGazeDataset} includes eye tracking data, segmentation maps, bounding boxes, dictation audio and its transcription. Moreover, REFLACX dataset \citep{Lanfredi2021REFLACX} offers another set of eye tracking information, anomaly locations ellipses, dictation audio and transcription. There are three main reasons why different modalities should be involved. Firstly, we argue that the model can be more gain better performance through receiving more information. Richer information can facilitate the classification process and boosts the robustness of model. Secondly, we believe human recognition pattern can be extracted from the fixation information. And, the fixation information itself or the extracted pattern can be used for improving both performance and explainability. At the last, some of the modalities can be served as both input and output. For instance, \citet{Karargyris2021EyeGazePaper} implement two experiments is their work. One of them feeds fixation information as input to boost the AUC. At another work, eye gaze data is used as a label in output section, which require the model to predict where the radiologists will look at and focus on. Generally, the sight of the radiologists can help us to localise the pathologies, which enhances the explainability of model. The overall framework has been shown in Figure \ref{fig: proposed_framework}. The modalities with dashed border can serve as input or output. Additionally, the four output modalities with blue background are those can provide explainability. These are: eye tracking information, report text, audio \& transcription and anomaly location ellipse.\\

% p.8 Aims of the PhD
This PhD targets on developing a multi-modal learning framework for medical images. The modalities that can be included in pre-training or training phase are chest X-ray image, report text, eye tracking information, utterance of report, segmentation and bounding boxes. Within the framework, an explaination method will be delivered to interpret the computational result of the model, which can helps human to comprehend how the model make decisions. Therefore, the overall function of the framework is to provide a system of explainable Automated chest radiograph interpretation. \\
\\
\\


\begin{figure}[!h]
    \centering
    \includegraphics[width=\textwidth]{./images/MultiModalLearningFramework.png}
    \caption{Overview of proposed framework.}
    \label{fig: proposed_framework}
\end{figure}

% \begin{table}[]
% \centering
\begin{longtable}{|m{10em}|m{30em}|}
    \hline
    Modality                                                                           & Example                                                                                            \\ \hline
    Chest X-ray \citep{Johnson2019MIMIC_CXR} \citep{DJohnson2019MIMIC_CXR_JPG}         & \begin{center}\includegraphics[width=.3\textwidth]{images/CXR-image.jpg}\end{center}               \\ \hline
    Report text \citep{Johnson2019MIMIC_CXR} \citep{DJohnson2019MIMIC_CXR_JPG}         & \begin{center}\includegraphics[width=.6\textwidth]{images/ReportText.png}\end{center}              \\ \hline
    Eye tracking data \citep{Karargyris2020EyeGazeDataset} \citep{Lanfredi2021REFLACX} & \begin{center}\includegraphics[width=.6\textwidth]{images/Fixation.png}\end{center}
    \\ \hline
    Segmentation of 4 key anatomical structures  \citep{Karargyris2020EyeGazeDataset}  & \begin{center}\includegraphics[width=.3\textwidth]{images/SegmentationMap.png}\end{center}
    \\ \hline
    Bounding boxes of 17 anatomical structures  \citep{Karargyris2020EyeGazeDataset}   & \begin{center}\includegraphics[width=.3\textwidth]{images/EyeGazeBoundingBox.png}\end{center}
    \\ \hline
    Anomaly location ellipses \citep{Lanfredi2021REFLACX}                              & \begin{center}\includegraphics[width=.3\textwidth]{images/AnomalyLocationEllipses.png}\end{center}
    \\ \hline
    Clinical Data  \citep{Johnson2021MIMIC_IV} \citep{Johnson2021MIMIC_IV_ED}          &
    patients' age, gender, temperature, heat rate, respiratory rate, oxygen saturation, systolic/ diastolic blood pressure, pain level, etc.                                                \\ \hline
    Dictation audio \citep{Karargyris2020EyeGazeDataset}                               & The audio recorded when the radiologists is interpreting chest X-ray images.
    \\ \hline
    Transcription \citep{Karargyris2020EyeGazeDataset} \citep{Lanfredi2021REFLACX}     & Recorded audio is automatically transcribed into a json file including word timestamp.             \\ \hline
    \caption{\textbf{Modalities can be included in this work.}} % put citation here.
    \label{tab:modalities_table}
\end{longtable}
% \end{table}


\subsection{Thesis Core Concepts}
The associated basic concpetes on which this project is based are explained in this section to provide the reader a clear understading of this research work:

\begin{itemize}
    \item \textbf{Itepretability and explainability:} \\ The difference between interpretability and explainability is subtle. When we assume that a model is interpretable, it often means that the model is a whitebox and can elaborate its reasoning. On the other hand, an explainable model is a blackbox model with an explanatory method to proivde insight into its decisions \citep{doshivelez2017RigorousInterpretable}. In most of the papers, however, they're used interchangeably. Hence, in this wrok, we will not ephasise the difference between them and consider them to be mutually replaceable. \\
    \item \textbf{Blackbox and whitebox models:} \\ The model based on pattern, rules or decision tree are considered white-box models, such as linear regression and logistic regression. These models provide their decision making process to users in a human comprehensible manner. On the other hand, in order to increase the capability of machine learning model, non-linearity and other complex mathematical functions have been added to the predicitive algorithms, which significantly damages the explainability \citep{LoyolaGonzalez2019BlackBoxVsWhiteBox}. Although  these models can fit complex distributions that result a higher performance on accuracy, the loss of interpretation raises reliability concerns.\\
    \item \textbf{Moti-modal learning:} \\ Modality refers to the way something happens or is experienced. And the word, \textit{sensory modalities}, stands for our primary communication and sensory channels such as seeing, hearing and touching. A research problem or a data setis therefore referred to as multimodal if it comprises several such modalities. Additionally, if a machine learning model has the ability to process related information from more than one modality, we consider it to be an implementation of multimodal learning \citep{Baltrušaitis2019MutimodalSurvey}. \\
          % \item \textbf{Multi-task learning:} \\ In general, a multi-task learning refers to the learning task where more than one loss function are used for optimization. The terms, \textit{joint learning}, \textit{learning to learn} and \textit{learning with auxiliary tasks} are only some other names of multi-task learning. Multi-modal learning is mainly used to improve the peformance of generalization throguh sharing the representations between related tasks \citep{Ruder2017MultitaskLearning}.\\
    \item \textbf{What else?}
\end{itemize}


%% 1. List all the literatures we can talk about. divide them in different section.

\subsection{Literature Review}

In order to gain a deep understanding of the existing work, a literature search is carried out to examine current theses. In this section, we categorise the literature into five different groups including (1) Explaiable AI (XAI), (2) Automated predictive system on Chest X-ray interpretation, (3) XAI on Medical Predictive Task, (4) Multi-Modal Learning (5) Eye Tracking Technology.

\subsubsection{Explainable AI (XAI)}

XAI is a set of methods that enable people to understand and trust the decisions made by machine learning algorithms. An overview of the XAI approach was clearly presented in the work of \citet{Belle2020XAIPriciples}. In the area of ​XAI, the approaches can initially be divided into transparent and opaque models, which are referred to as white box or black box models. The transparent models consist of the algorithms that are capable of self-reasoning. These whitebox models reveal their decision-making process or their decision-making boundaries to users once it has been trained. Since transparent models can explain themselves, an additional explanation algorithm is not necessary. On the flip side, the opaque model hardly provides any explanation for its output when it performs amazingly in terms of accuracy. In order to achieve desirable predictive performance while being reliable, most of the researches are conducted to explain these blackbox models. Post-hoc explainability is the method that is applied on a learned model without interfering the training process, which has the advatage not adversely affecting the model's performance \citep{Karlo2018XAISurvey} \citep{Belle2020XAIPriciples}. \\

The post-hoc explanations can be further seperated to two categories, inlclding \textit{model-agnostic} and \textit{model-specific}. The model-agnostic treat the original model as a black box and only require the input and output of the model to explain individual prediction \citep{Ribeiro2016Modelagnostic} \citep{Karlo2018XAISurvey} \citep{Ribeiro2018Anchors}. Some widely used model-agnostic methods are listed and introduced bewlow, which includes LIME\citep{Ribeiro2016LIME}, SHAP\citep{Lundberg2017SHAP}, LINDA-BN\citep{Moreira2021LINDA} and ANCHORS\citep{Ribeiro2018Anchors}:

\begin{itemize}
    \item \textbf{LIME:} \\ Local Interpretable Modal-agnostic Explaination (LIME) is to firstly generate a set of perturbed samples with associated lables around the input. And an interpretable model, such as decision or logistic regression, will be trained on the permutation dataset to approximate the original model in the local area. In another word, $f$ and $x$ are the original model and the input respectively. Then the LIME algorithm is trying to find an interpretable mode, $g$ to ensure $g(z^{'}) \approx f(h_{x}(z^{'}))$ where $z^{'} \approx x^{'}$. And $h_{x}$ is a mapping function $x = h_{x}(x^{'})$. The feature importance can therfore be extracted from the linear surrogate model to explain which important features leads to this decision, as shown in figure \ref{fig: LIME_ExampleOnCXR}.\\
    \item \textbf{SHAP:} \\ In 2017, \citet{Lundberg2017SHAP} unified 6 existing XAI methods that are using similar explanation method, including LIME, DeepLIFT, layer-wise relevance propagation, Shapley regression value, Shapley sampling value and Quantitative Input Influence. In that work, they propse SHapley Additive exPlanations (SHAP) values as a unified measure of feasture importance. It uses the Shapley value from game thoery to assign each feature an importance value for a particular predcition. At the point they're preparing the releasement, they also found a similar work pubished in economic field \citep{Charnes1988ShapeleyValueInEconomic}. \\
    \item \textbf{LINDA-BN:} \\ Local Interpretation-Driven Abstract Bayesian Network (LINDA-BN) is a framework using the probalistic graphical model to approximate the blackbox model in local area around input. Similiar to LIME, a local perturbed samples are generated with the prediction of the original as label to form a permutation dataset. Then this dataset will be used to train a Bayersian Network (BN). From the BN, it can explain about how each feature contribute to the result of this prediction, shown as Figure \ref{fig: LINDA-BN_diabetes}. Moreover, since the nature of the graphical model, correlations between input features can also be explored.  \citep{Moreira2021LINDA}.
    \item \textbf{ANCHORS:} \\ \citet{Ribeiro2018Anchors} argue that the explainer with ability to interpret locally is not ideal, as the coverage of these explainers is not clear. People tend to assume that the condition can be applied on other unseens instances, which is not always the case. The authors therefore proposed ANCHORS to fill the gap in existing explainer. ANCHORS is if-then rule-based algorithm to find the rule that sufficiently \textit{anchors} the prediction locally, which means that changing other features that are not anchors does not affect the prediction result. It can explain the behavoiur of complex mocels with high precision rules. The most obvious benefit is the clear coverage that is applied when the anchors are untouched.
\end{itemize}

On the other hand, model-specific approaches are those that require specific architecture of model in order to generate explaination. The main advantage of model specific methods is the facilitation of development on effeicnet algorithm based on the explaination we observed \citep{Belle2020XAIPriciples}. In the following, we will list out three model-spefic explainers that are widely used in computer vision task \citep{Spinner2020explAIner}, inlcuding GradCAM\citep{Selvaraju2017GradCAM}, Integrated Gradients\citep{Sundararajan2017IntegratedGradient} and DeepLIFT\citep{Shrikumar2017DeepLIFT}.


\begin{itemize}
    \item \textbf{Grad-CAM}: \\ Gradient-weighted Class Activation Mapping (Grad-CAM) is an explainer desinged specific for Convolutional Neural Network (CNN). It generalise CAM \citep{Zhou2015CAM} to be used on variety of CNN-based architecture. GradCAM uses back-propagation to track gradient in the last CNN layer to visualise the semantics it captured \citep{Selvaraju2017GradCAM}, show as Figure \ref{fig: GradCAMExample_OnCXR}. Another work, Grad-CAM++ \citep{Chattopadhay2018GradCAM}, further improve GradCAM through considiring penultimate layer of CNN and updating global avarage pooling. This improvement solve 2 issues on GradCAM, inlcuding (1) only part of the object can be localised and (2) object with multiple occurences can't be localised properly.
    \item \textbf{Integrated Gradients:} \\ \citet{Sundararajan2017IntegratedGradient} propsed 2 axioms that should be satisfied by every attribution method, inlcuding \textit{sensitivity} and \textit{Implementation of invariance}. The axiom of sensitivity can be satisfied if an function depend on some variables, then the attribution to those variables are always zero, and vice versa. And the axiom of inplementation invariance is satisfied when the outputs of two models are equal for all inputs, regardless the difference in implementation. And \textit{integrated gradient} is inspired by these two axioms. The most remarkable advantage of integrated gradient is that it doesn't require a specific model architecture as long as the model is differentiable. Integrated gradients are defined as the path intergral of the gradients along the straighline path from baseline to input.
    \item \textbf{DeepLIFT:} \\ Deep Learning Important FeaTure (DeepLIFT) \citep{Shrikumar2017DeepLIFT} uses back-propagation to decompose the output of the model and reveal the contribution of each input feature. Moreover, it discovers the depdencies between features through considering positive and negative contributions seperatelly. In order to address the problem of saturation, the \textbf{summation-to-delta} property (Eq. \ref{eq: summation-to-delta}) is proposed, where $C_{\Delta x_{i} \Delta t}$ is the amount of difference in output attributed to the difference of $x_{i}$.
\end{itemize}

\begin{equation} \label{eq: summation-to-delta}
    \sum_{i=1}^{n} C_{\Delta x_{i} \Delta t} = \Delta t
\end{equation}

\begin{figure}[!h]
    \centering
    \includegraphics[width=\textwidth]{./images/XAI-Overview.png}
    \caption{Overview of XAI. \citet{Belle2020XAIPriciples}}
    \label{fig: XAI_Overview}
\end{figure}

\subsubsection{Automated predictive system on Chest X-ray interpretation}

With the flourishing development on AI, the medical industry is beginning to explore the potential of machine learning to assist diagnostic  process. Chest X-ray is one of the safest and most frequent method to diagnosis patients; hence, the Computer-Aided Diagnosis (CAD) on CXR images is a popular field that researchers are investing. Before discussing the performance, the evaluation matrix should be defined. In the normal image classification tasks, accuracy is prefered matric to evaluate learning algorithms since \citep{Lobo2008AUDIsWrong} have proven the ineffectiveness of AUC on some cases. On the other hand, The area under ROC (receiver operating charactics) curve, as konwn as AUC, is used in medical diagnosis since 1970s \citep{Huang2005AUC}. Medical diagnosis require the model to have high recall and precision to minimise the amount of false-negative cases because it has more seroius consequence than a false-positive case. In order to mitigate patient's risk, most of medical predictive algorithms are evaluated by AUC \citep{HajianTilaki2013AUDOnMedical}. Since the difference in evaluating methods, the loss functions in medical fields are adapted, such as pairwise surrogate loss\citep{Gao2012SSurrogatelossAUC}, pairwise square loss\citep{Gao2013AUCSquareLoss}and Deep AUC Maximisation (DAM) \citep{Sulam2017MaximizingAUD}. In 2020, \citet{Yuan2020DAM} proprosed a new margin-based min-max surrogate loss function, which surpasses other models on the CheXpert\citep{Irvin2019Chexpert} till now. \\

Deep learning based apporaches has been proven efficient and effective in various tasks, especially on vision-related tasks \citep{Singh2020ExplainableMedicalImage}. In medical field, \citep{Ting2017DLOnEyeDisease} proposed a CNN-based model that is able to detect diabetic rentinopathy and ralated eye diseases by retinal images. \citet{Esteva2017DNNSkinCancer} shows that deep learning can reach dermatologist-level accuracy in terms of skin cancer detection. \citet{Peng2019DeepSeeNet} developed a 2 step framework, \textit{DeepSeeNet}, to predict the severity of age-related macular degeneration (AMD). This method first detect the risk factor causing this disease, such as drusen size and pigementary abnormalities. Then, it will assign a AMD severity score to each patient.

In some cases, the AI system can achieve expert-level performance or even outperform them on several visual tasks \citep{Rajpurkar2017CheXNet, Yuan2020DAM, Tang2020AbnormalityCXRCNN}.  Since automated chest x-ray interpretation can be considered as a image classification tasks, architectures and techniques from other domain can be resued. Transfer learning is a popular and effective method to improve the generalization on model \citep{Zhuang2019TransferLearningSurvey}. In general, the size of available CXR datasets are smaller than existing image datastes, such as MS-COCO \citep{Lin2014MSCOCO}, ImageNet \citep{Deng2009ImageNet} and OpenImage \citep{Kuznetsova2020OpenImages}. For example, \citet{Yuan2020DAM} and \citet{Cohen2020Covid19} use DenseNet\citep{Huang2016DenseNet} architecture. And, \citet{Dunnmon2019CNNOnCXRAssessment} use AlexNet\citep{Alex2012AlexNet}, ResNet \citep{He2015ResNet} and DenseNet pretrained on ImageNet database \citep{Deng2009ImageNet}. And \citet{Yates2018RedDotCXR} use pre-trained inception_v3\citep{Szegedy2015InceptionV3} from Google.

\subsubsection{XAI on Medical Predictive Task}

% ? Q: is is better to combine this and above?

Despite the popularity on medical AI field, only few methods are being used. Most of the works use CAM \citep{Zhou2015CAM}, GradCAM\citep{Selvaraju2017GradCAM} or other similar algorithms that generate silency map to explain diagnosis \citep{Teixeira2021LIMEAndGradCAMOnCXR, Rajpurkar2018CheXNeXt, Rajpurkar2017CheXNet, Brunese2020ExplainableCovid19, Hou2021ExplainableCovid19}. However, silency map can be unreliable at soem cases. Recently, explainable methods through visualising sailency maps are evaluated in the work of \citep{Saporta2021BechmarkingSaliencyMethods}. And the authors demonstrate that sailency maps consistently perform worse that human experts, regardless of the AUC of model.

\subsubsection{Muti-Modal Learning}
% \citep{Ruder2017MultitaskLearning}

% 1. What's multimodal learning => get the definition from paper.

% 2. What's the benefit we can gain from multi-modal learninig. 

% 3. What have been implemented in the deep learning area?

% 4. What have been implemented in the medical area?


Integrating such heterogeneous data to form a holistic space.

\subsubsection{Eye Tracking Technology}

% 1. explain how eye tracking works and  what kind of information can be captured by eye tracking data. 

Eye tracking technology has been studited since a long time ago. In 1950, researchers studited pilots' eye movements through a head-mounted eye-tracking device. The findings of this research are eventually used for redesigning the cockpit, which improves usability and recudes the risk caused by human error \citep{Fitts2005Cockpit, Cooke2005EyeTrackingUsability}.

Today, most eye tracking technologies use corneal reflection, which uses a light source to illuminate the eye. This action creates a little glint under the pupil that the device can use to calibrate and track eye movement \citep{Schall2014EyeTrackingIntro, Fitts2005Cockpit}.

\citet{Blascheck2014VisualisingEyeTracking} illustrates that A temporal order of \textit{fixation} within an Area Of interest (AOI) can be viewed as a \textit{gaze}. The duration of the fixation is usually represented by the radius. \textit{Saccades} are the connections between fixations. When a saccade connects two AOIs it's considered as  \textit{transition}. Finally, \textit{scanpath} consists of a complete sequence of fixations and saccades. The authors also generate human attention map to visualise human's interest on certain scene. This kind of application can assist UI workers to re-design and improve user experience \citep{Schall2014EyeTrackingIntro}.

% 3. cases that prove eye tracking is useful for capturign user's cognitive process.
When we're trying to capture where's users attention, it's unreliable to let user describe the whole story. In the work of \citet{Guan2006ThinkAloud}, the author demonstrated that human can reach 47\% of omissions in think-aloud experiment, which represents that human can't self-report precisely. Eye tracking technology is one of the effective methods to track human attention. The eye movement and human cognition are inextricably. \citep{Schall2014EyeTrackingIntro} states that eye tracking technology enable us to capture cognitive processes and visual perception. And, it can be potentially used to observe people's emotions and intentions \citep{Harezlak2018EyeTrackingInMedicine}. \citet{Rayner2015EvidenceCongnitiveFixation} demonstrates the evidence that direct cognitive control of eye fixations in reading. Another prove that eye tracking is highly relatable to cognitive is that eye tracking allow use to detect disease with congnitive disorders \citep{Harezlak2018EyeTrackingInMedicine}, such as Parkinson's disease \citep{S2016Parkinson}, Alzheimer's disease \citep{Crawford2015Alzheimer},
Schizophrenia \citep{Levy2010Schizophrenia} and Autism \citep{Belen2021Autism}.


While some studies suggest that the link between cognition and eye is not always correct \citep{Anderson2004EyeMovementNegativeSupport, Schall2014EyeTrackingIntro}, it's generally applicalbe that human tend to move the eyes to the stimulus that they're currently thinking about or processing \citep{Cater2020BestPracticeEyeTracking}. And behaviour that congnition guides the eye movement is called eye-mind link \citep{Reichle2012EyeMindLink1, Reichle2010EyeMindLink2}.

% 4. eye tracking cases on CXR dataset.
Eye tracking is also a popular tool used in medical industry. \citet{Manning2003ExpreienceRadiologist} and \citet{Nicholas2015PassingGlance} use eye tracking technology to show that radiologist with different experience have distinct difference in terms of scanning and interpreting strategies. As the eye tracking infomration can indicate the interpretation performance, \citep{McLaughlin2017EvaluateRadiologistPeformanceLevel} proposed an algorithm to assess radiologists' interpretation technique. \citet{Karargyris2021EyeGazePaper} implemented two experiments to demonstrate how can eye tracking information benefit deep learning based predictive system. It shows the eye tracking data can be considered as both input and label for the model. When eye tracking data is treated as input of the model, it can boost the accuracy of the model since it can the information from radiologist. And it can also considered as a output of model to improve the genrarlisation and provide explanation. Recently, \citep{Saab2021EyeTrackingCXRClassification} also proposed a similar algorithms to use eye tracking data as labels in a mutli-task learning framework. In addition, the authors shows the gaze freatures from normal and abnormal CXR images differ noticeably. A set of weak lables can therefore be extracted from eye tracking data for supervised-learning tasks.


\begin{figure}[!h]
    \centering
    \includegraphics[width=.8\textwidth]{./images/ThisWorkCover-2.png}
    \caption{Venn diagram of this work.}
    \label{fig: this_work_cover}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=.8\textwidth]{./images/LIME_ExampleOnCXR.png}
    \caption{LIME example on Chest X-ray images. \citep{Teixeira2021LIMEAndGradCAMOnCXR}}
    \label{fig: LIME_ExampleOnCXR}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=.8\textwidth]{./images/GradCAMExample_OnCXR.png}
    \caption{GradCAM example on Chest X-ray images. \citep{Teixeira2021LIMEAndGradCAMOnCXR}}
    \label{fig: GradCAMExample_OnCXR}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=.8\textwidth]{./images/LINDA-BN.png}
    \caption{LINDA-BN explains why a patient is predcited not having diabetes. \citep{Moreira2021LINDA}}
    \label{fig: LINDA-BN_diabetes}
\end{figure}


%% List the sections that I need to do literature review on.


\subsection{Research Problem and Research Questions}

\underline{\textbf{Scope}} This work scope to the creation of multimodal learnining framework, which can recieve chest X-ray images, clinical data and eye tracking data as input to make a diagnois. \ref{fig: this_work_cover} shows the 3 main areas of this work. Our analysis will be based on public datasets \citep{Johnson2021MIMIC_IV, Johnson2019MIMIC_CXR, Johnson2021MIMIC_IV_ED, Lanfredi2021REFLACX, Karargyris2020EyeGazeDataset}. And, only the Disease , Congestive Heart Failure (CHF) and Pneumonia, are within the scope.

\noindent
\underline{\textbf{Problem Statement.}}
In a practical case, the radiologist commonly use clinical data to assist their process of diagnosis. Recent literature also suggests that clinical data can benefit the accuracy of diagnosis \citep{Castillo2020ClinicalInformationOnRadiology, Leslie2000CTClinicalData}. However, most of the existing works in medical AI only use medical images to make a diagnosis. Focusing only on this pixel-level classification can lead to bias, which does not precisely substantiate the diagnosis by radiologists. Ignoring the bias in an AI model can increase the risk for minority groups in terms of discrimination and poor diagnosis. Motivated by this real-world need we identified the following research gaps, which will lead to the proposed research goals.

\begin{itemize}
    \item \textbf{\underline{Research Gap 1.} The combination of clinical data and eye tracking data can promote more accurate and interpretable model} \\ Recent literature suggests that using accurate patients' \textit{clinical data} together with medical images can improve the performance of human radiologist in terms of accuracy. \citep{Castillo2020ClinicalInformationOnRadiology, Leslie2000CTClinicalData}. Additionally, there are studies suggesting that \textit{eye tracking data} from radiologists can shed light on how radiologists interpret Chest X-ray images and obtain diagnoses \citep{Karargyris2021EyeGazePaper, Lanfredi2021REFLACX}. However, to the best of our knowledge, no work in the existing literature has attempted to propose predictive architectures using these two modalities. It's also uncertain whether these modalities can promote more interpretable models.
          \begin{itemize}
              \item \textbf{Research Question 1:} \\ How can eye tracking data representation mechanism increase the radiologist' understandability and interpretation of a CXR image.
                    %   What form of eye tracking data representation mechanism can promote human understanding as radiologist assess the lesion in CXR.
              \item \textbf{Research Question 2:} \\  What's are the most appropriate multi-modal architectures that are required to process different types of data, such as eye tracking data (time-series data), chest X-ray (image data) and clinical data (tabular).
          \end{itemize}
    \item  \textbf{\underline{Research Gap 2.} The usage of cognitive load theory on multi-modal classification for chest X-ray images is under-explored in the literature.} There's a strong body of literature suggesting a correlation between fixation points from eye tracking data and cognitive load. And, the duration of the fixation can represent the difficulty of the task \citep{Palinko2010EyeTrackingCognitiveLoad-1, Zagermann2016EyeTrackingCognitiveLoad-2, Wang2014EyeTrackingCognitiveLoad-3, Krejtz2018EyeTrackingCognitiveLoad-4,Klingner2010EyeTrackingCognitiveLoad-5}. However, only a small number of studies have explored the potential of using the cognitive load theory feature for diagnosis in CXR images.
          \begin{itemize}
              \item \textbf{Research Question 3:} \\ What features can we extract from eye tracking data in order to get insight into the level of cognitive fatigue and the difficulty of the task in chest X-ray diagnosis?
              \item \textbf{Research Question 4:} \\ Can cognitive load features be used in the neural network to identify bias due to fatigue or difficulty?
          \end{itemize}
    \item \textbf{\underline{Research Gap 3.} Static Explainable Interfaces Fail at Promoting Engagement and system interpretability in Radiologists.} The explanation user interface consists of the sum of the outputs of an XAI process that the user can directly interact with \citep{Chromik2021}. Most studies in explainable AI focus on static computational aspects of generating explanations (explanations generated by LIME or SHAP) while limited research is reported concerning the human-centered design of the explainable interface. There is a growing need in the literature in the design of interactive and dynamic interfaces that allow the user to drill down or ask for different types of explanations until he is satisfied. Recently, \citet{Chromik2021} suggests different interactive interface for XAI, including interaction as control and interaction as dialog. These two interaction methods allow the framework adjust its output according to the feedback from the user, which provides most interactive experience to users.
          \begin{itemize}
              \item \textbf{Research Question 5:} \\ What are the user requirements to make this explainable interface interpretable to radiologists and to help them learn how can this system help them how to make classification.
              \item \textbf{Research Question 6:} \\ What kind of interaction are useful to promote understand-ability in radiologists of how and why the predictive system make its diagnosis.
          \end{itemize}
\end{itemize}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% %% \noindent
% \textbf{\underline{Research Gap 2.} Deep Learning Classifiers Make Classifications Differently from Human Radiologists.} Most deep learning architectures in the literature make classifications using uniquely pixels of X-ray images and other image-based features that are not understandable to humans. This leads to the generation of erroneous explanations that are different from human radiologists and to biases that can put minority societal groups in danger of discrimination and poor diagnosis.

% \item \textbf{Research Gap 3:} There are no standard human-centric evaluation protocols for explainable AI, specially for application-grounded and human-grounded evaluations. %Different authors perform the evaluation of their approaches based on different measures. %There is a need for a unified evaluation protocol for XAI.

% Personally conducted interviews with radiologists in Portugal suggest that the radiolo-gists need the patient’s clinical data to make accurate readings and diagnoses from an X-ray image. Thisis the primary reason why the interviewed radiologists distrust AI models: one cannot make an accuratediagnosis by looking uniquely at X-ray images since one cannot understand if a lesion that is in the imagecorresponds to a scar from an old pulmonary disease or whether it is a consequence of a current pathologythat the patient has. Motivated by this real-world need we identified the following research gaps, which willlead to the proposed research goals.

% %% \noindent
% \textbf{\underline{Research Gap 2.} Deep Learning Classifiers Make Classifications Differently from Human Radiologists.} Most deep learning architectures in the literature make classifications using uniquely pixels of X-ray images and other image-based features that are not understandable to humans. This leads to the generation of erroneous explanations that are different from human radiologists and to biases that can put minority societal groups in danger of discrimination and poor diagnosis.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% 1. Most of the works in the literature use only images to predict diagnoisis. focusing on this pixel-level classification can lead to bias, which is not accureatly reasoning how radiologist diagnosis. Howeve, recent literature suggest

% %% \noindent
% \textbf{\underline{Research Gap 2.} Deep Learning Classifiers Make Classifications Differently from Human Radiologists.} Most deep learning architectures in the literature make classifications using uniquely pixels of X-ray images and other image-based features that are not understandable to humans. This leads to the generation of erroneous explanations that are different from human radiologists and to biases that can put minority societal groups in danger of discrimination and poor diagnosis.

% \item \textbf{Research Gap 3:} There are no standard human-centric evaluation protocols for explainable AI, specially for application-grounded and human-grounded evaluations. %Different authors perform the evaluation of their approaches based on different measures. %There is a need for a unified evaluation protocol for XAI.

%  \begin{itemize}
%     \item \textbf{RQ 4:} 
%     What is an evaluation protocol that can be used effectively assess the quality (structure) and the believability of the explanation generated from the persuasive system?
%     %How the evaluation experiment can be designed to achieve evaluating the believability of the explanation instead of focusing too much on user mental change? 

%     %*Existing evaluaiton methods/ protocal to measure the human-understanding / relevance of user understandable evaluations 
%     %*Reasonable scale ... .

%   % Determining what makes an explanation better than another is one of the most pressing open research questions in XAI literature. By proposing a standardized evaluation protocol for explainability, one can promote the reliability of both interpretable and explainable models, increasing the user's trust in the system. 
% \end{itemize}

% Research Question 1:
% What kind of eye tracking data representation machinism can promote human understating in radiologist in the assessement of the lesion in CXR.

% Research Question 2: 
% what kind of multi-modal architecture can support eye tracking data and clinical data.

%%%%% Since the goal of this project is to come out interpretability, how do radiologist perceive eye tracking data? Does it provide understanding to the radiologist? 


\noindent

